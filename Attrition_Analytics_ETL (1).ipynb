{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Attrition_modelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import configparser\n",
    "import calendar\n",
    "import datetime as dt\n",
    "import bisect\n",
    "import xlsxwriter\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# suppress the warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a log file\n",
    "log= open(\"Attrition Analytics Log.txt\",\"a\")\n",
    "log.write(\"********** Opened the log file on  ***********\\n\")\n",
    "orig_date= datetime.now()\n",
    "date_str = datetime.strftime(orig_date, '%Y-%m-%d %H:%M:%S')\n",
    "log.write(date_str)\n",
    "log.write(\"\\n\")\n",
    "log.write(\"Executing Attrition Analytics ETL\\r\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the quarter start dates\n",
    "# today = dt.date.today()\n",
    "# qbegins = []\n",
    "# for yr in [today.year-1, today.year]:\n",
    "#     if yr == today.year-1:\n",
    "#         qbegins = [dt.date(yr, month, 1) for month in (4, 7, 10)]\n",
    "#         qbegins1 = qbegins\n",
    "#     else:\n",
    "#         qbegins = [dt.date(yr, month, 1) for month in (1, 4, 7, 10)]\n",
    "#         qbegins = qbegins + qbegins1\n",
    "        \n",
    "# idx = bisect.bisect(qbegins, today)\n",
    "# qbegins = [qbegins[idx-1], qbegins[idx-2], qbegins[idx-3], qbegins[idx-4]]\n",
    "# qbegins = pd.to_datetime(qbegins)\n",
    "\n",
    "# qbegins\n",
    "\n",
    "# # get the quarter end dates\n",
    "# qends = []\n",
    "# for yr in [today.year-1, today.year]:\n",
    "#     if yr == today.year-1:\n",
    "#         qends = [dt.date(yr, month, 30) for month in (6, 9, 12)]\n",
    "#         qends1 = qends\n",
    "#     else:\n",
    "#         qends = [dt.date(yr, month, 30) for month in (3, 6, 9, 12)]\n",
    "#         qends = qends + qends1\n",
    "        \n",
    "# idx = bisect.bisect(qends, today)\n",
    "# qends = [qends[idx],qends[idx-1], qends[idx-2], qends[idx-3]]\n",
    "# qends = pd.to_datetime(qends)\n",
    "# qends\n",
    "\n",
    "# # logic to handle 31st day in a month\n",
    "# qends1 = []\n",
    "# for i in range(len(qends)):\n",
    "#     if qends[i].month in (3, 12):\n",
    "#         qends1.append((qends[i] + timedelta(days=1)))\n",
    "#     else:\n",
    "#         qends1.append(qends[i]) \n",
    "# qends1 = pd.to_datetime(qends1)\n",
    "\n",
    "# #defining year quarter \n",
    "# yr_qtr1  = str(qbegins.to_period('Q')[0])\n",
    "# yr_qtr2  = str(qbegins.to_period('Q')[1])\n",
    "# yr_qtr3  = str(qbegins.to_period('Q')[2])\n",
    "# yr_qtr4  = str(qbegins.to_period('Q')[3])\n",
    "\n",
    "# # converting to datetime\n",
    "# qbegins = qbegins.to_pydatetime()\n",
    "# qends1 = qends1.to_pydatetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     # get the folder name\n",
    "#     folder = datetime.now().strftime('%b %Y')\n",
    "\n",
    "#     # define quarter start and end dates\n",
    "#     q1_start = qbegins[0]\n",
    "#     q1_end = qends1[0]\n",
    "    \n",
    "#     q2_start = qbegins[1]\n",
    "#     q2_end = qends1[1]\n",
    "\n",
    "#     q3_start = qbegins[2]\n",
    "#     q3_end = qends1[2]\n",
    "    \n",
    "#     q4_start = qbegins[3]\n",
    "#     q4_end = qends1[3]\n",
    "\n",
    "#     print(\"Successfully defined quarters\\r\\n\")\n",
    "#     log.write(\"Successfully defined quarters\\r\\n\")\n",
    "    \n",
    "# except:\n",
    "#     print(\"Failed to define quarters\\r\\n\")\n",
    "#     log.write(\"Failed to define quarters\\r\\n\")\n",
    "#     log.close()\n",
    "#     sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the folder name\n",
    "# folder = datetime.now().strftime('%b %Y')\n",
    "\n",
    "# q1_start = pd.datetime(2020,7,1)\n",
    "# q1_end = pd.datetime(2020,9,30)\n",
    "\n",
    "# q2_start = pd.datetime(2020,4,1)\n",
    "# q2_end = pd.datetime(2020,6,30)\n",
    "\n",
    "# q3_start = pd.datetime(2020,1,1)\n",
    "# q3_end = pd.datetime(2020,3,31)\n",
    "\n",
    "# q4_start = pd.datetime(2019,10,1)\n",
    "# q4_end = pd.datetime(2019,12,31)\n",
    "\n",
    "# yr_qtr1  = '2020Q3'\n",
    "# yr_qtr2  = '2020Q2'\n",
    "# yr_qtr3  = '2020Q1'\n",
    "# yr_qtr4  = '2019Q4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read Config file\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config_parser.ini')\n",
    "\n",
    "# folder name\n",
    "folder = config['folder']['folder']\n",
    "\n",
    "# define quarter start and end dates\n",
    "yr_qtr1  = config['quarter definition']['yr_qtr1']\n",
    "q1_start = pd.datetime(int(config['quarter definition']['y_q1_start']), int(config['quarter definition']['m_q1_start']), int(config['quarter definition']['d_q1_start']))\n",
    "q1_end = pd.datetime(int(config['quarter definition']['y_q1_end']), int(config['quarter definition']['m_q1_end']), int(config['quarter definition']['d_q1_end']))\n",
    "\n",
    "yr_qtr2  = config['quarter definition']['yr_qtr2']\n",
    "q2_start = pd.datetime(int(config['quarter definition']['y_q2_start']), int(config['quarter definition']['m_q2_start']), int(config['quarter definition']['d_q2_start']))\n",
    "q2_end = pd.datetime(int(config['quarter definition']['y_q2_end']), int(config['quarter definition']['m_q2_end']), int(config['quarter definition']['d_q2_end']))\n",
    "\n",
    "yr_qtr3  = config['quarter definition']['yr_qtr3']\n",
    "q3_start = pd.datetime(int(config['quarter definition']['y_q3_start']), int(config['quarter definition']['m_q3_start']), int(config['quarter definition']['d_q3_start']))\n",
    "q3_end = pd.datetime(int(config['quarter definition']['y_q3_end']), int(config['quarter definition']['m_q3_end']), int(config['quarter definition']['d_q3_end']))\n",
    "\n",
    "yr_qtr4  = config['quarter definition']['yr_qtr4']\n",
    "q4_start = pd.datetime(int(config['quarter definition']['y_q4_start']), int(config['quarter definition']['m_q4_start']), int(config['quarter definition']['d_q4_start']))\n",
    "q4_end = pd.datetime(int(config['quarter definition']['y_q4_end']), int(config['quarter definition']['m_q4_end']), int(config['quarter definition']['d_q4_end']))\n",
    "\n",
    "print(\"Successfully read Config file\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020Q3\n",
      "2019Q4\n"
     ]
    }
   ],
   "source": [
    "print(yr_qtr1)\n",
    "print(yr_qtr4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes date range(e.g. trip start date and trip end date) and assigns the days into respective quarters\n",
    "def quarter_days_allocator(data, cols, q_start, q_end):\n",
    "    return (np.where(\n",
    "                      ((data[cols[0]] < q_start) &  (data[cols[1]] > q_end)), \n",
    "                            data[cols[2]],\n",
    "                                np.where(\n",
    "                                        ((data[cols[0]] < q_start) & ((data[cols[1]]  >= q_start) & (data[cols[1]]  <= q_end))),\n",
    "                                                data[cols[3]],\n",
    "                                                    np.where(\n",
    "                                                            (((data[cols[0]] >= q_start) & (data[cols[0]] <= q_end)) & (data[cols[1]] > q_end)),\n",
    "                                                                    data[cols[4]],\n",
    "                                                                            np.where(\n",
    "                                                                                    (((data[cols[0]] >= q_start) & (data[cols[0]] <= q_end)) & ((data[cols[1]] >= q_start) & (data[cols[1]] <= q_end))),\n",
    "                                                                                            data[cols[5]],\n",
    "                                                                                                    0)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\Employee Comprehensive Report\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Masked Code</th>\n",
       "      <th>DATE_OF_BIRTH</th>\n",
       "      <th>Masked Gender</th>\n",
       "      <th>MARITAL_STATUS</th>\n",
       "      <th>WEDDING_DATE</th>\n",
       "      <th>Masked Location</th>\n",
       "      <th>PERMNANT_STATE</th>\n",
       "      <th>Masked Entity</th>\n",
       "      <th>Masked Status</th>\n",
       "      <th>Masked Department</th>\n",
       "      <th>...</th>\n",
       "      <th>NOTICE_PERIOD_IN_DAYS</th>\n",
       "      <th>DATE_OF_JOINING</th>\n",
       "      <th>DATE_OF_LEAVING</th>\n",
       "      <th>Masked RM ID</th>\n",
       "      <th>PARTNER_NAME</th>\n",
       "      <th>REPORTING_TO</th>\n",
       "      <th>EMPLOYEE_STATUS</th>\n",
       "      <th>AGE</th>\n",
       "      <th>YEARS_SINCE_MARRIAGE</th>\n",
       "      <th>YEARS_AT_COMPANY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91EShF/nnLUqp3e+hCL9ag==</td>\n",
       "      <td>1988-12-10</td>\n",
       "      <td>S002</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaT</td>\n",
       "      <td>L014</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>E011</td>\n",
       "      <td>ES001</td>\n",
       "      <td>SL083</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>2017-01-16</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>//u+WyHm0RVCzJMGRkQAJg==</td>\n",
       "      <td>Jaskiran Bhatia</td>\n",
       "      <td>Mallarswami R Nonvinkere</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>31.806266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.704388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fTVAyZmScwfarHFnb7uxcg==</td>\n",
       "      <td>1989-07-26</td>\n",
       "      <td>S002</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaT</td>\n",
       "      <td>L004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E011</td>\n",
       "      <td>ES001</td>\n",
       "      <td>SL083</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>2017-05-26</td>\n",
       "      <td>//u+WyHm0RVCzJMGRkQAJg==</td>\n",
       "      <td>Jaskiran Bhatia</td>\n",
       "      <td>Mallarswami R Nonvinkere</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>31.182023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.531900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1qJ7mSYktnFiPm11FXL6+Q==</td>\n",
       "      <td>1991-04-01</td>\n",
       "      <td>S001</td>\n",
       "      <td>Married</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>L014</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>E011</td>\n",
       "      <td>ES001</td>\n",
       "      <td>SL083</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>2017-09-08</td>\n",
       "      <td>//u+WyHm0RVCzJMGRkQAJg==</td>\n",
       "      <td>Jaskiran Bhatia</td>\n",
       "      <td>Mallarswami R Nonvinkere</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>29.500948</td>\n",
       "      <td>4.432671</td>\n",
       "      <td>3.646892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Zs6nmpo53o6spqg1984dg==</td>\n",
       "      <td>1983-07-01</td>\n",
       "      <td>S001</td>\n",
       "      <td>Married</td>\n",
       "      <td>2013-05-29</td>\n",
       "      <td>L007</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>E011</td>\n",
       "      <td>ES002</td>\n",
       "      <td>SL173</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>NaT</td>\n",
       "      <td>/+mkiC7vIkQFVDBYg0ER5Q==</td>\n",
       "      <td>Kedar Anil Sawale</td>\n",
       "      <td>Johar Ali Batterywala</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>37.251963</td>\n",
       "      <td>7.340329</td>\n",
       "      <td>10.738071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j4u0ELqrgMb62Vmncb5QwQ==</td>\n",
       "      <td>1975-07-12</td>\n",
       "      <td>S001</td>\n",
       "      <td>Married</td>\n",
       "      <td>2004-12-26</td>\n",
       "      <td>L014</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>E011</td>\n",
       "      <td>ES002</td>\n",
       "      <td>SL173</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>2010-04-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>/+mkiC7vIkQFVDBYg0ER5Q==</td>\n",
       "      <td>Sachin Chandrakant Rasam</td>\n",
       "      <td>Johar Ali Batterywala</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>45.222010</td>\n",
       "      <td>15.762131</td>\n",
       "      <td>10.442377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rS2aNvJKd9r3uBZQFp5nUA==</td>\n",
       "      <td>1981-03-22</td>\n",
       "      <td>S002</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaT</td>\n",
       "      <td>L014</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>E011</td>\n",
       "      <td>ES002</td>\n",
       "      <td>SL173</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>2006-02-13</td>\n",
       "      <td>NaT</td>\n",
       "      <td>/+mkiC7vIkQFVDBYg0ER5Q==</td>\n",
       "      <td>Senthilvel Kaliyamurthy</td>\n",
       "      <td>Johar Ali Batterywala</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>39.527163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.628637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OuiPsJYG58cScUbQG7IELA==</td>\n",
       "      <td>1989-06-26</td>\n",
       "      <td>S001</td>\n",
       "      <td>Married</td>\n",
       "      <td>2018-04-05</td>\n",
       "      <td>L004</td>\n",
       "      <td>Tamil nadu</td>\n",
       "      <td>E011</td>\n",
       "      <td>ES002</td>\n",
       "      <td>SL002</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>2016-06-13</td>\n",
       "      <td>2019-11-29</td>\n",
       "      <td>/+mkiC7vIkQFVDBYg0ER5Q==</td>\n",
       "      <td>Deepa Seshadri</td>\n",
       "      <td>Johar Ali Batterywala</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>31.264160</td>\n",
       "      <td>2.488757</td>\n",
       "      <td>4.298514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8KXH9PWiEIntEtXZfxXl2Q==</td>\n",
       "      <td>1988-08-25</td>\n",
       "      <td>S002</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaT</td>\n",
       "      <td>L004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E011</td>\n",
       "      <td>ES002</td>\n",
       "      <td>SL145</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>2014-11-10</td>\n",
       "      <td>2015-11-04</td>\n",
       "      <td>/+mkiC7vIkQFVDBYg0ER5Q==</td>\n",
       "      <td>Sachin Narayan Paranjape</td>\n",
       "      <td>Johar Ali Batterywala</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>32.099222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.889238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S/WXSWtz/WjMJiJu30dNdg==</td>\n",
       "      <td>1984-10-04</td>\n",
       "      <td>S001</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaT</td>\n",
       "      <td>L004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E013</td>\n",
       "      <td>ES002</td>\n",
       "      <td>SL153</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>2010-04-05</td>\n",
       "      <td>2011-08-17</td>\n",
       "      <td>/+mkiC7vIkQFVDBYg0ER5Q==</td>\n",
       "      <td>Santosh Kumar</td>\n",
       "      <td>Johar Ali Batterywala</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>35.989788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.488922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nyvMhOhbC5KRL+pr6qa35A==</td>\n",
       "      <td>1984-04-01</td>\n",
       "      <td>S001</td>\n",
       "      <td>Married</td>\n",
       "      <td>2012-02-12</td>\n",
       "      <td>L004</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>E011</td>\n",
       "      <td>ES002</td>\n",
       "      <td>SL002</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>/+mkiC7vIkQFVDBYg0ER5Q==</td>\n",
       "      <td>Johar Ali Batterywala</td>\n",
       "      <td>Johar Ali Batterywala</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>36.499038</td>\n",
       "      <td>8.632621</td>\n",
       "      <td>5.672943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Masked Code DATE_OF_BIRTH Masked Gender MARITAL_STATUS  \\\n",
       "0  91EShF/nnLUqp3e+hCL9ag==    1988-12-10          S002         Single   \n",
       "1  fTVAyZmScwfarHFnb7uxcg==    1989-07-26          S002         Single   \n",
       "2  1qJ7mSYktnFiPm11FXL6+Q==    1991-04-01          S001        Married   \n",
       "3  /Zs6nmpo53o6spqg1984dg==    1983-07-01          S001        Married   \n",
       "4  j4u0ELqrgMb62Vmncb5QwQ==    1975-07-12          S001        Married   \n",
       "5  rS2aNvJKd9r3uBZQFp5nUA==    1981-03-22          S002         Single   \n",
       "6  OuiPsJYG58cScUbQG7IELA==    1989-06-26          S001        Married   \n",
       "7  8KXH9PWiEIntEtXZfxXl2Q==    1988-08-25          S002         Single   \n",
       "8  S/WXSWtz/WjMJiJu30dNdg==    1984-10-04          S001         Single   \n",
       "9  nyvMhOhbC5KRL+pr6qa35A==    1984-04-01          S001        Married   \n",
       "\n",
       "  WEDDING_DATE Masked Location PERMNANT_STATE Masked Entity Masked Status  \\\n",
       "0          NaT            L014    Maharashtra          E011         ES001   \n",
       "1          NaT            L004            NaN          E011         ES001   \n",
       "2   2016-04-25            L014    Maharashtra          E011         ES001   \n",
       "3   2013-05-29            L007        Haryana          E011         ES002   \n",
       "4   2004-12-26            L014    Maharashtra          E011         ES002   \n",
       "5          NaT            L014    Maharashtra          E011         ES002   \n",
       "6   2018-04-05            L004     Tamil nadu          E011         ES002   \n",
       "7          NaT            L004            NaN          E011         ES002   \n",
       "8          NaT            L004            NaN          E013         ES002   \n",
       "9   2012-02-12            L004      Karnataka          E011         ES002   \n",
       "\n",
       "  Masked Department  ... NOTICE_PERIOD_IN_DAYS DATE_OF_JOINING  \\\n",
       "0             SL083  ...                    30      2017-01-16   \n",
       "1             SL083  ...                    30      2017-03-20   \n",
       "2             SL083  ...                    30      2017-02-06   \n",
       "3             SL173  ...                    60      2010-01-04   \n",
       "4             SL173  ...                    60      2010-04-22   \n",
       "5             SL173  ...                    60      2006-02-13   \n",
       "6             SL002  ...                    60      2016-06-13   \n",
       "7             SL145  ...                    60      2014-11-10   \n",
       "8             SL153  ...                    60      2010-04-05   \n",
       "9             SL002  ...                    60      2015-01-28   \n",
       "\n",
       "   DATE_OF_LEAVING              Masked RM ID              PARTNER_NAME  \\\n",
       "0       2017-04-21  //u+WyHm0RVCzJMGRkQAJg==           Jaskiran Bhatia   \n",
       "1       2017-05-26  //u+WyHm0RVCzJMGRkQAJg==           Jaskiran Bhatia   \n",
       "2       2017-09-08  //u+WyHm0RVCzJMGRkQAJg==           Jaskiran Bhatia   \n",
       "3              NaT  /+mkiC7vIkQFVDBYg0ER5Q==         Kedar Anil Sawale   \n",
       "4              NaT  /+mkiC7vIkQFVDBYg0ER5Q==  Sachin Chandrakant Rasam   \n",
       "5              NaT  /+mkiC7vIkQFVDBYg0ER5Q==   Senthilvel Kaliyamurthy   \n",
       "6       2019-11-29  /+mkiC7vIkQFVDBYg0ER5Q==            Deepa Seshadri   \n",
       "7       2015-11-04  /+mkiC7vIkQFVDBYg0ER5Q==  Sachin Narayan Paranjape   \n",
       "8       2011-08-17  /+mkiC7vIkQFVDBYg0ER5Q==             Santosh Kumar   \n",
       "9       2019-11-11  /+mkiC7vIkQFVDBYg0ER5Q==     Johar Ali Batterywala   \n",
       "\n",
       "               REPORTING_TO EMPLOYEE_STATUS        AGE YEARS_SINCE_MARRIAGE  \\\n",
       "0  Mallarswami R Nonvinkere        INACTIVE  31.806266                  NaN   \n",
       "1  Mallarswami R Nonvinkere        INACTIVE  31.182023                  NaN   \n",
       "2  Mallarswami R Nonvinkere        INACTIVE  29.500948             4.432671   \n",
       "3     Johar Ali Batterywala          ACTIVE  37.251963             7.340329   \n",
       "4     Johar Ali Batterywala          ACTIVE  45.222010            15.762131   \n",
       "5     Johar Ali Batterywala          ACTIVE  39.527163                  NaN   \n",
       "6     Johar Ali Batterywala        INACTIVE  31.264160             2.488757   \n",
       "7     Johar Ali Batterywala        INACTIVE  32.099222                  NaN   \n",
       "8     Johar Ali Batterywala        INACTIVE  35.989788                  NaN   \n",
       "9     Johar Ali Batterywala        INACTIVE  36.499038             8.632621   \n",
       "\n",
       "   YEARS_AT_COMPANY  \n",
       "0          3.704388  \n",
       "1          3.531900  \n",
       "2          3.646892  \n",
       "3         10.738071  \n",
       "4         10.442377  \n",
       "5         14.628637  \n",
       "6          4.298514  \n",
       "7          5.889238  \n",
       "8         10.488922  \n",
       "9          5.672943  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# importing employee data and filtering the required data\n",
    "employee_data = pd.read_excel('Employee Data Comprehensive Report.xlsx', sheet_name = \"Sheet1\")\n",
    "\n",
    "\n",
    "employee_data = employee_data[(employee_data['Masked Grade'] != \"G012\") & \n",
    "                          (employee_data['Masked Grade'] != \"G013\") &\n",
    "                          (employee_data['Masked Grade'] != \"G014\") &\n",
    "                          (employee_data['Masked Grade'] != \"G017\") &\n",
    "                          (employee_data['Masked Grade'] != \"G007\")]\n",
    "\n",
    "employee_data = employee_data[['Masked emp code.'\n",
    "                           ,'DATE_OF_BIRTH'\n",
    "                           ,'Masked gender'\n",
    "                           ,'MARITAL_STATUS'\n",
    "                           ,'WEDDING_DATE'\n",
    "                           ,'Masked Location'\n",
    "                           ,'PERMNANT_STATE'\n",
    "                           ,'Masked Entity'\n",
    "                           ,'Masked emp status'\n",
    "                           ,'Masked SL'\n",
    "                           ,'Masked Function'\n",
    "                           ,'Masked Grade'\n",
    "                           ,'NOTICE_PERIOD_IN_DAYS'\n",
    "                           ,'DATE_OF_JOINING'\n",
    "                           ,'DATE_OF_LEAVING'\n",
    "                           ,'Masked RM Ids'\n",
    "                           ,'PARTNER_NAME'\n",
    "                           ,'REPORTING_TO'\n",
    "                           ,'EMPLOYEE_STATUS']]\n",
    "\n",
    "# rename the columns\n",
    "employee_data = employee_data.rename(columns={\"Masked emp code.\": \"Masked Code\",\n",
    "                                        \"Masked gender\": \"Masked Gender\",\n",
    "                                        \"Masked emp status\": \"Masked Status\",\n",
    "                                        \"Masked SL\": \"Masked Department\",\n",
    "                                        \"Masked RM Ids\": \"Masked RM ID\"})\n",
    "\n",
    "employee_data['DATE_OF_BIRTH'] = pd.to_datetime(employee_data['DATE_OF_BIRTH'])\n",
    "employee_data['WEDDING_DATE'] = pd.to_datetime(employee_data['WEDDING_DATE'])\n",
    "employee_data['DATE_OF_JOINING'] = pd.to_datetime(employee_data['DATE_OF_JOINING'])\n",
    "employee_data['DATE_OF_LEAVING'] = pd.to_datetime(employee_data['DATE_OF_LEAVING'])\n",
    "\n",
    "# calculate age\n",
    "employee_data['AGE'] = q1_end - employee_data['DATE_OF_BIRTH']\n",
    "employee_data['AGE'] = employee_data['AGE']/np.timedelta64(1,'Y')\n",
    "\n",
    "# calculate years since marriage\n",
    "employee_data['YEARS_SINCE_MARRIAGE'] = q1_end - employee_data['WEDDING_DATE']\n",
    "employee_data['YEARS_SINCE_MARRIAGE'] = employee_data['YEARS_SINCE_MARRIAGE']/np.timedelta64(1,'Y')\n",
    "\n",
    "# calculate tenure at Deloitte\n",
    "employee_data['YEARS_AT_COMPANY'] = q1_end - employee_data['DATE_OF_JOINING']\n",
    "employee_data['YEARS_AT_COMPANY'] = employee_data['YEARS_AT_COMPANY']/np.timedelta64(1,'Y')\n",
    "    \n",
    "employee_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27960"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(employee_data['Masked Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code/Code/Mapping Table/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully mapped Office location\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import the location mapping table and map it with the employee data to get the actual office location\n",
    "location_mapping = pd.read_excel('Office Location Code Mapping.xlsx', sheet_name = \"Sheet1\")\n",
    "\n",
    "\n",
    "employee_data = pd.merge(employee_data, location_mapping, on='Masked Location', how='left')\n",
    "\n",
    "# IsSameState \n",
    "employee_data['IsSameState'] = np.where(employee_data['Office State'] == employee_data['PERMNANT_STATE'], 1, 0)\n",
    "\n",
    "print(\"Successfully mapped Office location\\r\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\Pay Group/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed Pay Group file\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# importing paygroup data to eliminate non-permanent employees\n",
    "paygroup_data = pd.read_excel('Pay Group.xlsx', sheet_name = \"Sheet1\")\n",
    "\n",
    "paygroup_data['NonPermanent_flag'] = 1\n",
    "paygroup_data.columns = ['Masked Code','NonPermanent_flag']\n",
    "employee_data = pd.merge(employee_data, paygroup_data, on='Masked Code', how='left')\n",
    "employee_data = employee_data[employee_data['NonPermanent_flag'] != 1]\n",
    "\n",
    "del employee_data['NonPermanent_flag']\n",
    "\n",
    "print(\"Successfully executed Pay Group file\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\Resignation Status Report\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed Resignation data\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resignation_data = pd.read_excel('Resignation Status Report.xlsx', sheet_name = \"Sheet1\") \n",
    "\n",
    "\n",
    "resignation_data = resignation_data[resignation_data['SEPARATIONTYPE']== \"RESIGNATION\"]\n",
    "resignation_data = resignation_data[resignation_data['STATUS']== \"Approved\"]\n",
    "resignation_data = resignation_data[['Masked emp id.','RESIGNATIONLETTERDATE']]\n",
    "resignation_data.columns = ['Masked Code', 'Resignation Date']\n",
    "\n",
    "resignation_data['Resignation Date'] = pd.to_datetime(resignation_data['Resignation Date'])\n",
    "\n",
    "employee_data = pd.merge(employee_data, resignation_data, on='Masked Code', how='left')\n",
    "\n",
    "employee_data['SeparationFlag']  = np.where(((employee_data['Resignation Date'].isnull()) & (employee_data['EMPLOYEE_STATUS'] == 'INACTIVE')), 1, 0)\n",
    "\n",
    "employee_data = employee_data[employee_data['SeparationFlag'] == 0]\n",
    "del employee_data['SeparationFlag']\n",
    "\n",
    "print(\"Successfully executed Resignation data\\r\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(employee_data.loc[employee_data['Attrition']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(employee_data['Resignation Date'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully defined active employees in considered Quarter\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA = employee_data\n",
    "#DATA = employee_data[employee_data['EMPLOYEE_STATUS']== 'ACTIVE']\n",
    "\n",
    "# creating Class label for Attrition\n",
    "# DATA['Attrition'] = np.where((DATA['Resignation Date'] <= q1_end), 1, 0)\n",
    "DATA['Attrition'] = np.where(((DATA['Resignation Date'] >= q1_start) & (DATA['Resignation Date'] <= q1_end)), 1, 0)\n",
    "DATA['']\n",
    "# keep only active employees\n",
    "# DATA = DATA[DATA['Attrition']==0]\n",
    "\n",
    "print(\"Successfully defined active employees in considered Quarter\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10353"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DATA.loc[DATA['Attrition'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\Performance Grouping Data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed Performance Grouping Data\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "performance_data = pd.read_excel('Performance Grouping Data.xlsx', sheet_name = \"Sheet1\")\n",
    "\n",
    "\n",
    "\n",
    "performance_data.columns = ['Encrypted Emp ID', 'Performance Group']\n",
    "\n",
    "# check if we have multiple records\n",
    "check_performance_data = performance_data.groupby(['Encrypted Emp ID'], as_index=False).count()\n",
    "check_performance_data = check_performance_data.sort_values(by=['Performance Group'], ascending=False)\n",
    "\n",
    "if ((check_performance_data.iloc[0, 1]) >1):\n",
    "    print(\"Data Quality Issue - Performance data has duplicate values\")\n",
    "\n",
    "\n",
    "# enrich with performance data\n",
    "# performance rating of employees\n",
    "DATA = pd.merge(DATA, performance_data, left_on='Masked Code', right_on='Encrypted Emp ID', how='left')\n",
    "DATA = DATA.rename(columns={'Performance Group':'EmpPerformanceRating'}) \n",
    "\n",
    "# performance rating of RM's\n",
    "DATA = pd.merge(DATA, performance_data, left_on='Masked RM ID', right_on='Encrypted Emp ID', how='left')\n",
    "DATA = DATA.rename(columns={'Performance Group':'RMPerformanceRating'})\n",
    "\n",
    "print(\"Successfully executed Performance Grouping Data\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\Career Progression Report\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed Career Progression Report\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "promotion_data = pd.read_excel(\"Career Progression Report.xlsx\", sheet_name = \"Sheet1\")\n",
    "\n",
    "\n",
    "\n",
    "# remove only designation changes\n",
    "promotion_data = promotion_data[promotion_data['Re-designation'].isnull()]\n",
    "\n",
    "\n",
    "# subset the data to have the required structure\n",
    "promotion_data_G007 = promotion_data[['Masked Emp Id', 'G007 since', 'DOJ']]\n",
    "promotion_data_G007.columns = ['Masked Code', 'EffectiveDate', 'DOJ']\n",
    "promotion_data_G007['Grade'] = 'G007'\n",
    "\n",
    "promotion_data_G003 = promotion_data[['Masked Emp Id', 'G003 Since', 'DOJ']]\n",
    "promotion_data_G003.columns = ['Masked Code', 'EffectiveDate', 'DOJ']\n",
    "promotion_data_G003['Grade'] = 'G003'\n",
    "\n",
    "promotion_data_G009 = promotion_data[['Masked Emp Id', 'G009 Since', 'DOJ']]\n",
    "promotion_data_G009.columns = ['Masked Code', 'EffectiveDate', 'DOJ']\n",
    "promotion_data_G009['Grade'] = 'G009'\n",
    "\n",
    "promotion_data_G006 = promotion_data[['Masked Emp Id', 'G006 Since.2', 'DOJ']]\n",
    "promotion_data_G006.columns = ['Masked Code', 'EffectiveDate', 'DOJ']\n",
    "promotion_data_G006['Grade'] = 'G006'\n",
    "\n",
    "promotion_data_G004 = promotion_data[['Masked Emp Id', 'G004 Since', 'DOJ']]\n",
    "promotion_data_G004.columns = ['Masked Code', 'EffectiveDate', 'DOJ']\n",
    "promotion_data_G004['Grade'] = 'G004'\n",
    "\n",
    "promotion_data_G001 = promotion_data[['Masked Emp Id', 'G001 Since', 'DOJ']]\n",
    "promotion_data_G001.columns = ['Masked Code', 'EffectiveDate', 'DOJ']\n",
    "promotion_data_G001['Grade'] = 'G001'\n",
    "\n",
    "promotion_data_G008 = promotion_data[['Masked Emp Id', 'G008 Since', 'DOJ']]\n",
    "promotion_data_G008.columns = ['Masked Code', 'EffectiveDate', 'DOJ']\n",
    "promotion_data_G008['Grade'] = 'G008'\n",
    "\n",
    "\n",
    "# concat the datasets\n",
    "promotion_data = pd.concat([promotion_data_G007, promotion_data_G003, promotion_data_G009, promotion_data_G006,\n",
    "                            promotion_data_G004, promotion_data_G001, promotion_data_G008], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# replace DOJ string with the date of joining\n",
    "promotion_data['EffectiveDate'] = np.where((promotion_data['EffectiveDate'] == 'Since DOJ'), promotion_data['DOJ'], promotion_data['EffectiveDate'])\n",
    "\n",
    "del promotion_data['DOJ']\n",
    "\n",
    "# format the date\n",
    "promotion_data['EffectiveDate'] = pd.to_datetime(promotion_data['EffectiveDate'], errors='coerce').dt.date\n",
    "promotion_data['EffectiveDate'] = pd.to_datetime(promotion_data['EffectiveDate'])\n",
    "\n",
    "# drop the rows with null values\n",
    "promotion_data = promotion_data.dropna()\n",
    "\n",
    "# sort the data\n",
    "promotion_data = promotion_data.sort_values(by=['Masked Code', 'EffectiveDate'], ascending=False)\n",
    "\n",
    "promotion_data['IsPromoted'] = 'Promoted'\n",
    "\n",
    "# enrich with promotion data\n",
    "promotion_data = promotion_data[promotion_data['EffectiveDate'] <= q1_end]\n",
    "promotion_data = promotion_data.drop_duplicates(subset = 'Masked Code', keep = 'first')\n",
    "\n",
    "#flag for current and previous quarter promotion\n",
    "promotion_data['IsPromoted_LatestQtr'] = np.where((promotion_data['EffectiveDate'] >= q1_start) ,1, 0)\n",
    "promotion_data['IsPromoted_PreviousQtr'] = np.where(((promotion_data['EffectiveDate'] >= q2_start) & (promotion_data['EffectiveDate'] <= q2_end)) ,1, 0)                                                      \n",
    "\n",
    "DATA = pd.merge(DATA, promotion_data, on='Masked Code', how='left')\n",
    "\n",
    "print(\"Successfully executed Career Progression Report\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\Staff Utilization Report\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed Staff Utilization Data\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import the quarterly utilization data\n",
    "util_data_Q5 = pd.read_excel('Staff Utilization Report_Q5.xlsx', sheet_name = \"Sheet1\")\n",
    "util_data_Q5 = util_data_Q5[['Masked Code', 'Standard Hours', 'Billable Hours', 'Productive Non-Billable Hours', 'Non-Productive Non-Billable Hours']]\n",
    "util_data_Q5['Util_Flag'] = \"SUR_Q5\"\n",
    "util_data_Q5.columns = ['Masked Code', 'Standard Hours', 'Billable Hours', 'Productive Non-Billable Hours', 'Non-Productive Non-Billable Hours', 'Util_Flag']\n",
    "\n",
    "util_data_Q4 = pd.read_excel('Staff Utilization Report_Q4.xlsx', sheet_name = \"Sheet1\")\n",
    "util_data_Q4 = util_data_Q4[['Masked Code', 'Standard Hours', 'Billable Hours', 'Productive Non-Billable Hours', 'Non-Productive Non-Billable Hours']]\n",
    "util_data_Q4['Util_Flag'] = \"SUR_Q4\"\n",
    "util_data_Q4.columns = ['Masked Code', 'Standard Hours', 'Billable Hours', 'Productive Non-Billable Hours', 'Non-Productive Non-Billable Hours', 'Util_Flag']\n",
    "\n",
    "util_data_Q3 = pd.read_excel('Staff Utilization Report_Q3.xlsx', sheet_name = \"Sheet1\")\n",
    "util_data_Q3 = util_data_Q3[['Masked Code', 'Standard Hours', 'Billable Hours', 'Productive Non-Billable Hours', 'Non-Productive Non-Billable Hours']]\n",
    "util_data_Q3['Util_Flag'] = \"SUR_Q3\"\n",
    "util_data_Q3.columns = ['Masked Code', 'Standard Hours', 'Billable Hours', 'Productive Non-Billable Hours', 'Non-Productive Non-Billable Hours', 'Util_Flag']\n",
    "\n",
    "util_data_Q2 = pd.read_excel('Staff Utilization Report_Q2.xlsx', sheet_name = \"Sheet1\")\n",
    "util_data_Q2 = util_data_Q2[['Masked Code', 'Standard Hours', 'Billable Hours', 'Productive Non-Billable Hours', 'Non-Productive Non-Billable Hours']]\n",
    "util_data_Q2['Util_Flag'] = \"SUR_Q2\"\n",
    "util_data_Q2.columns = ['Masked Code', 'Standard Hours', 'Billable Hours', 'Productive Non-Billable Hours', 'Non-Productive Non-Billable Hours', 'Util_Flag']\n",
    "\n",
    "util_data_Q1 = pd.read_excel('Staff Utilization Report_Q1.xlsx', sheet_name = \"Sheet1\")\n",
    "util_data_Q1 = util_data_Q1[['Masked Code', 'Standard Hours', 'Billable Hours', 'Productive Non-Billable Hours', 'Non-Productive Non-Billable Hours']]\n",
    "util_data_Q1['Util_Flag'] = \"SUR_Q1\"\n",
    "util_data_Q1.columns = ['Masked Code', 'Standard Hours', 'Billable Hours', 'Productive Non-Billable Hours', 'Non-Productive Non-Billable Hours', 'Util_Flag']\n",
    "\n",
    "\n",
    "# concate all the quarterly utilization datasets\n",
    "staff_util_data = pd.concat([util_data_Q5\n",
    "                        ,util_data_Q4\n",
    "                        ,util_data_Q3\n",
    "                        ,util_data_Q2\n",
    "                        ,util_data_Q1], axis=0)\n",
    "\n",
    "\n",
    "# aggregate at employee and quarter level\n",
    "staff_util_data = staff_util_data.groupby(['Masked Code', 'Util_Flag'], as_index=False).agg({'Standard Hours':'sum'\n",
    "                                                                                          ,'Billable Hours':'sum'\n",
    "                                                                                          ,'Productive Non-Billable Hours':'sum'\n",
    "                                                                                          ,'Non-Productive Non-Billable Hours':'sum'})\n",
    "\n",
    "\n",
    "\n",
    "# enrich with utilization data\n",
    "# by lastest year\n",
    "staff_util_data_latestYr = staff_util_data[(staff_util_data['Util_Flag'] == \"SUR_Q1\")|\n",
    "                                           (staff_util_data['Util_Flag'] == \"SUR_Q2\")| \n",
    "                                           (staff_util_data['Util_Flag'] == \"SUR_Q3\")|\n",
    "                                           (staff_util_data['Util_Flag'] == \"SUR_Q4\")|\n",
    "                                           (staff_util_data['Util_Flag'] == \"SUR_Q5\")]\n",
    "\n",
    "\n",
    "# formulae to adjust for quarter definition\n",
    "staff_util_data_latestYr['Standard Hours'] = np.where((staff_util_data_latestYr['Util_Flag'] == \"SUR_Q1\"),\n",
    "                                                    (staff_util_data_latestYr['Standard Hours'] * 0.33),\n",
    "                                                         np.where((staff_util_data_latestYr['Util_Flag'] == \"SUR_Q5\"),\n",
    "                                                                  (staff_util_data_latestYr['Standard Hours'] * 0.66),\n",
    "                                                                      staff_util_data_latestYr['Standard Hours']\n",
    "                                                ))\n",
    "\n",
    "staff_util_data_latestYr['Billable Hours'] = np.where((staff_util_data_latestYr['Util_Flag'] == \"SUR_Q1\"),\n",
    "                                                    (staff_util_data_latestYr['Billable Hours'] * 0.33),\n",
    "                                                         np.where((staff_util_data_latestYr['Util_Flag'] == \"SUR_Q5\"),\n",
    "                                                                  (staff_util_data_latestYr['Billable Hours'] * 0.66),\n",
    "                                                                      staff_util_data_latestYr['Billable Hours']\n",
    "                                                ))\n",
    "\n",
    "staff_util_data_latestYr['Productive Non-Billable Hours'] = np.where((staff_util_data_latestYr['Util_Flag'] == \"SUR_Q1\"),\n",
    "                                                    (staff_util_data_latestYr['Productive Non-Billable Hours'] * 0.33),\n",
    "                                                         np.where((staff_util_data_latestYr['Util_Flag'] == \"SUR_Q5\"),\n",
    "                                                                  (staff_util_data_latestYr['Productive Non-Billable Hours'] * 0.66),\n",
    "                                                                      staff_util_data_latestYr['Productive Non-Billable Hours']\n",
    "                                                ))\n",
    "\n",
    "staff_util_data_latestYr['Non-Productive Non-Billable Hours'] = np.where((staff_util_data_latestYr['Util_Flag'] == \"SUR_Q1\"),\n",
    "                                                    (staff_util_data_latestYr['Non-Productive Non-Billable Hours'] * 0.33),\n",
    "                                                         np.where((staff_util_data_latestYr['Util_Flag'] == \"SUR_Q5\"),\n",
    "                                                                  (staff_util_data_latestYr['Non-Productive Non-Billable Hours'] * 0.66),\n",
    "                                                                      staff_util_data_latestYr['Non-Productive Non-Billable Hours']\n",
    "                                                ))\n",
    "\n",
    "staff_util_data_latestYr = staff_util_data_latestYr.groupby(['Masked Code'], as_index=False).agg({'Billable Hours':'sum'\n",
    "                                                                                                      ,'Productive Non-Billable Hours':'sum'\n",
    "                                                                                                      ,'Non-Productive Non-Billable Hours':'sum'\n",
    "                                                                                                     ,'Standard Hours':'sum'})\n",
    "\n",
    "# derive metrics\n",
    "staff_util_data_latestYr['BILL_UTIL'] = staff_util_data_latestYr['Billable Hours']/staff_util_data_latestYr['Standard Hours']\n",
    "staff_util_data_latestYr['PROD_NONBILL_UTIL'] = staff_util_data_latestYr['Productive Non-Billable Hours']/staff_util_data_latestYr['Standard Hours']\n",
    "staff_util_data_latestYr['NONPROD_NONBILL_UTIL'] = staff_util_data_latestYr['Non-Productive Non-Billable Hours']/staff_util_data_latestYr['Standard Hours']\n",
    "\n",
    "staff_util_data_latestYr = staff_util_data_latestYr[['Masked Code','BILL_UTIL','PROD_NONBILL_UTIL','NONPROD_NONBILL_UTIL']]\n",
    "staff_util_data_latestYr.columns = ['Masked Code', 'LatestYr_BILL_UTIL', 'LatestYr_PROD_NONBILL_UTIL', 'LatestYr_NONPROD_NONBILL_UTIL']\n",
    "\n",
    "DATA = pd.merge(DATA, staff_util_data_latestYr, on='Masked Code', how='left')\n",
    "\n",
    "\n",
    "# by lastest quarter\n",
    "staff_util_data_latestQtr = staff_util_data[(staff_util_data['Util_Flag'] == \"SUR_Q1\") |\n",
    "                                            (staff_util_data['Util_Flag'] == \"SUR_Q2\")]\n",
    "\n",
    "# formulae to adjust for quarter definition\n",
    "staff_util_data_latestQtr['Standard Hours'] = np.where((staff_util_data_latestQtr['Util_Flag'] == \"SUR_Q1\"),\n",
    "                                                    (staff_util_data_latestQtr['Standard Hours'] * 0.33),\n",
    "                                                         (staff_util_data_latestQtr['Standard Hours'] * 0.66)\n",
    "                                                )\n",
    "\n",
    "staff_util_data_latestQtr['Billable Hours'] = np.where((staff_util_data_latestQtr['Util_Flag'] ==  \"SUR_Q1\"),\n",
    "                                                            (staff_util_data_latestQtr['Billable Hours'] * 0.33),\n",
    "                                                                 (staff_util_data_latestQtr['Billable Hours'] * 0.66)\n",
    "                                                )\n",
    "\n",
    "staff_util_data_latestQtr['Productive Non-Billable Hours'] = np.where((staff_util_data_latestQtr['Util_Flag'] ==  \"SUR_Q1\"),\n",
    "                                                                (staff_util_data_latestQtr['Productive Non-Billable Hours'] * 0.33),\n",
    "                                                                     (staff_util_data_latestQtr['Productive Non-Billable Hours'] * 0.66)\n",
    "                                                )\n",
    "\n",
    "staff_util_data_latestQtr['Non-Productive Non-Billable Hours'] = np.where((staff_util_data_latestQtr['Util_Flag'] ==  \"SUR_Q1\"),\n",
    "                                                                (staff_util_data_latestQtr['Non-Productive Non-Billable Hours'] * 0.33),\n",
    "                                                                     (staff_util_data_latestQtr['Non-Productive Non-Billable Hours'] * 0.66)\n",
    "                                                )\n",
    "\n",
    "staff_util_data_latestQtr = staff_util_data_latestQtr.groupby(['Masked Code'], as_index=False).agg({'Billable Hours':'sum'\n",
    "                                                                                                      ,'Productive Non-Billable Hours':'sum'\n",
    "                                                                                                      ,'Non-Productive Non-Billable Hours':'sum'\n",
    "                                                                                                     ,'Standard Hours':'sum'})\n",
    "\n",
    "# derive metrics\n",
    "staff_util_data_latestQtr['BILL_UTIL'] = staff_util_data_latestQtr['Billable Hours']/staff_util_data_latestQtr['Standard Hours']\n",
    "staff_util_data_latestQtr['PROD_NONBILL_UTIL'] = staff_util_data_latestQtr['Productive Non-Billable Hours']/staff_util_data_latestQtr['Standard Hours']\n",
    "staff_util_data_latestQtr['NONPROD_NONBILL_UTIL'] = staff_util_data_latestQtr['Non-Productive Non-Billable Hours']/staff_util_data_latestQtr['Standard Hours']\n",
    "\n",
    "staff_util_data_latestQtr = staff_util_data_latestQtr[['Masked Code','BILL_UTIL','PROD_NONBILL_UTIL','NONPROD_NONBILL_UTIL']]\n",
    "staff_util_data_latestQtr.columns = ['Masked Code', 'LatestQtr_BILL_UTIL', 'LatestQtr_PROD_NONBILL_UTIL', 'LatestQtr_NONPROD_NONBILL_UTIL']\n",
    "\n",
    "DATA = pd.merge(DATA, staff_util_data_latestQtr, on='Masked Code', how='left')\n",
    "\n",
    "print(\"Successfully executed Staff Utilization Data\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\Past Employment Data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed Past Employment Data\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "past_employment_data = pd.read_excel('Past Employment Data for Active and Inactive Practitioners.xlsx', sheet_name = \"Sheet1\", header=1)\n",
    "\n",
    "\n",
    "# subset required columns\n",
    "past_employment_data = past_employment_data[['Masked Code'\n",
    "                                    ,'From', 'To', 'Exp. in Years and Months'\n",
    "                                    ,'From.1', 'To.1', 'Exp. in Years and Months.1'\n",
    "                                    ,'From.2', 'To.2', 'Exp. in Years and Months.2'\n",
    "                                    ,'From.3', 'To.3', 'Exp. in Years and Months.3'\n",
    "                                    ,'From.4', 'To.4', 'Exp. in Years and Months.4'\n",
    "                                    ,'From.5', 'To.5', 'Exp. in Years and Months.5'\n",
    "                                    ,'From.6', 'To.6', 'Exp. in Years and Months.6'\n",
    "                                    ,'From.7', 'To.7', 'Exp. in Years and Months.7'\n",
    "                                    ,'From.8', 'To.8', 'Exp. in Years and Months.8'\n",
    "                                    ,'From.9', 'To.9', 'Exp. in Years and Months.9'\n",
    "                                    ,'From.10', 'To.10', 'Exp. in Years and Months.10'\n",
    "                                    ,'From.11', 'To.11', 'Exp. in Years and Months.11'\n",
    "                                    ,'From.12', 'To.12', 'Exp. in Years and Months.12'\n",
    "                                    ,'From.13', 'To.13', 'Exp. in Years and Months.13'\n",
    "                                    ,'Deloitte Exp', 'Total Exp.']]\n",
    "\n",
    "# calculate number of prior companies worked\n",
    "past_employment_data = past_employment_data.assign(NUM_PRIOR_COMPANIES = past_employment_data.loc[:, 'From':'Exp. in Years and Months.13'].notna().sum(axis=1))\n",
    "past_employment_data['NUM_PRIOR_COMPANIES'] = past_employment_data['NUM_PRIOR_COMPANIES']/3\n",
    "\n",
    "# change the date format\n",
    "past_employment_data[['From','To'\n",
    "            ,'From.1', 'To.1'\n",
    "            ,'From.2', 'To.2'\n",
    "            ,'From.3', 'To.3'\n",
    "            ,'From.4', 'To.4'\n",
    "            ,'From.5', 'To.5'\n",
    "            ,'From.6', 'To.6'\n",
    "            ,'From.7', 'To.7'\n",
    "            ,'From.8', 'To.8'\n",
    "            ,'From.9', 'To.9'\n",
    "            ,'From.10', 'To.10'\n",
    "            ,'From.11', 'To.11'\n",
    "            ,'From.12', 'To.12'\n",
    "            ,'From.13', 'To.13'\n",
    "            ]] = past_employment_data[['From','To'\n",
    "                                    ,'From.1', 'To.1'\n",
    "                                    ,'From.2', 'To.2'\n",
    "                                    ,'From.3', 'To.3'\n",
    "                                    ,'From.4', 'To.4'\n",
    "                                    ,'From.5', 'To.5'\n",
    "                                    ,'From.6', 'To.6'\n",
    "                                    ,'From.7', 'To.7'\n",
    "                                    ,'From.8', 'To.8'\n",
    "                                    ,'From.9', 'To.9'\n",
    "                                    ,'From.10', 'To.10'\n",
    "                                    ,'From.11', 'To.11'\n",
    "                                    ,'From.12', 'To.12'\n",
    "                                    ,'From.13', 'To.13']].apply(pd.to_datetime)\n",
    "\n",
    "# replace the missing dates with a default/dummy value for easier feature calculation\n",
    "past_employment_data[['From','To'\n",
    "            ,'From.1', 'To.1'\n",
    "            ,'From.2', 'To.2'\n",
    "            ,'From.3', 'To.3'\n",
    "            ,'From.4', 'To.4'\n",
    "            ,'From.5', 'To.5'\n",
    "            ,'From.6', 'To.6'\n",
    "            ,'From.7', 'To.7'\n",
    "            ,'From.8', 'To.8'\n",
    "            ,'From.9', 'To.9'\n",
    "            ,'From.10', 'To.10'\n",
    "            ,'From.11', 'To.11'\n",
    "            ,'From.12', 'To.12'\n",
    "            ,'From.13', 'To.13'\n",
    "            ]] = past_employment_data[['From','To'\n",
    "                                    ,'From.1', 'To.1'\n",
    "                                    ,'From.2', 'To.2'\n",
    "                                    ,'From.3', 'To.3'\n",
    "                                    ,'From.4', 'To.4'\n",
    "                                    ,'From.5', 'To.5'\n",
    "                                    ,'From.6', 'To.6'\n",
    "                                    ,'From.7', 'To.7'\n",
    "                                    ,'From.8', 'To.8'\n",
    "                                    ,'From.9', 'To.9'\n",
    "                                    ,'From.10', 'To.10'\n",
    "                                    ,'From.11', 'To.11'\n",
    "                                    ,'From.12', 'To.12'\n",
    "                                    ,'From.13', 'To.13']].replace(np.nan, pd.datetime(1900,1,1))\n",
    "\n",
    "# calculate average tenure in prior companies\n",
    "past_employment_data['PRIOR_EXP'] = ((past_employment_data['To'] - past_employment_data['From']) \n",
    "                          + (past_employment_data['To.1'] - past_employment_data['From.1']) \n",
    "                          + (past_employment_data['To.2'] - past_employment_data['From.2']) \n",
    "                          + (past_employment_data['To.3'] - past_employment_data['From.3']) \n",
    "                          + (past_employment_data['To.4'] - past_employment_data['From.4']) \n",
    "                          + (past_employment_data['To.5'] - past_employment_data['From.5']) \n",
    "                          + (past_employment_data['To.6'] - past_employment_data['From.6']) \n",
    "                          + (past_employment_data['To.7'] - past_employment_data['From.7']) \n",
    "                          + (past_employment_data['To.8'] - past_employment_data['From.8'])\n",
    "                          + (past_employment_data['To.9'] - past_employment_data['From.9'])\n",
    "                          + (past_employment_data['To.10'] - past_employment_data['From.10'])\n",
    "                          + (past_employment_data['To.11'] - past_employment_data['From.11'])\n",
    "                          + (past_employment_data['To.12'] - past_employment_data['From.12'])\n",
    "                          + (past_employment_data['To.13'] - past_employment_data['From.13']))\n",
    "\n",
    "past_employment_data['PRIOR_EXP'] = past_employment_data['PRIOR_EXP']/np.timedelta64(1,'Y')\n",
    "\n",
    "past_employment_data['PRIOR_AVG_TENURE'] = past_employment_data['PRIOR_EXP']/past_employment_data['NUM_PRIOR_COMPANIES']\n",
    "\n",
    "# create a column for Experienced/Fresher Flag\n",
    "past_employment_data['IS_EXPERIENCED'] = 1\n",
    "\n",
    "# keep only required columns\n",
    "past_employment_data = past_employment_data[['Masked Code'\n",
    "                                    ,'IS_EXPERIENCED'\n",
    "                                    ,'NUM_PRIOR_COMPANIES'\n",
    "                                    ,'PRIOR_EXP'\n",
    "                                    ,'PRIOR_AVG_TENURE'\n",
    "                                    ,'From'\n",
    "                                    ,'From.1'\n",
    "                                    ,'From.2'\n",
    "                                    ,'From.3'\n",
    "                                    ,'From.4'\n",
    "                                    ,'From.5'\n",
    "                                    ,'From.6'\n",
    "                                    ,'From.7'\n",
    "                                    ,'From.8'\n",
    "                                    ,'From.9'\n",
    "                                    ,'From.10'\n",
    "                                    ,'From.11'\n",
    "                                    ,'From.12'\n",
    "                                    ,'From.13'\n",
    "                                    ]]\n",
    "\n",
    "# enrich with past employment data\n",
    "DATA = pd.merge(DATA, past_employment_data, on='Masked Code', how='left')\n",
    "DATA['IS_EXPERIENCED'] = DATA['IS_EXPERIENCED'].fillna(0)\n",
    "\n",
    "# calculate total working experience (including Deloitte Experience)\n",
    "DATA['TOTAL_WORK_EXP'] = DATA['PRIOR_EXP'] + DATA['YEARS_AT_COMPANY']\n",
    "\n",
    "# calculate the number of companies an employee worked in last 5 years\n",
    "DATA['LAST5YRS_CUTOFF_DATE'] = q1_end - pd.DateOffset(years = 5)\n",
    "\n",
    "# replace the missing dates with a default/dummy value for easier feature calculation for freshers\n",
    "DATA[['From'\n",
    ",'From.1'\n",
    ",'From.2'\n",
    ",'From.3'\n",
    ",'From.4'\n",
    ",'From.5'\n",
    ",'From.6'\n",
    ",'From.7'\n",
    ",'From.8'\n",
    ",'From.9'\n",
    ",'From.10'\n",
    ",'From.11'\n",
    ",'From.12'\n",
    ",'From.13'\n",
    "]] = DATA[['From'\n",
    ",'From.1'\n",
    ",'From.2'\n",
    ",'From.3'\n",
    ",'From.4'\n",
    ",'From.5'\n",
    ",'From.6'\n",
    ",'From.7'\n",
    ",'From.8'\n",
    ",'From.9'\n",
    ",'From.10'\n",
    ",'From.11'\n",
    ",'From.12'\n",
    ",'From.13']].replace(np.nan, pd.datetime(1900,1,1))\n",
    "\n",
    "DATA['LAST5YRS_CUTOFF_DATE'] = pd.to_datetime(DATA['LAST5YRS_CUTOFF_DATE']) #, errors='coerce'\n",
    "\n",
    "DATA['LAST5YRS_COMP_WORKED'] = DATA[['From'\n",
    "                            ,'From.1'\n",
    "                            ,'From.2'\n",
    "                            ,'From.3'\n",
    "                            ,'From.4'\n",
    "                            ,'From.5'\n",
    "                            ,'From.6'\n",
    "                            ,'From.7'\n",
    "                            ,'From.8'\n",
    "                            ,'From.9'\n",
    "                            ,'From.10'\n",
    "                            ,'From.11'\n",
    "                            ,'From.12'\n",
    "                            ,'From.13']].ge(DATA.loc[:, 'LAST5YRS_CUTOFF_DATE'], axis = 0).sum(axis = 1)\n",
    "\n",
    "DATA['LAST5YRS_COMP_WORKED'] = np.where((DATA['IS_EXPERIENCED'] == 1), \n",
    "                                              (DATA['LAST5YRS_COMP_WORKED'] + 1),\n",
    "                                                  DATA['LAST5YRS_COMP_WORKED'])\n",
    "\n",
    "print(\"Successfully executed Past Employment Data\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\LnD Data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed LnD Data\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lnd_data = pd.read_excel('LnD Data.xlsx', sheet_name = \"Sheet1\")\n",
    "\n",
    "\n",
    "\n",
    "lnd_data['Completion Date'] = pd.to_datetime(lnd_data['Completion Date'])\n",
    "lnd_data = lnd_data[['Masked Code','Category','Learning Hours','Completion Date']]\n",
    "\n",
    "# remove Compliance trainings\n",
    "lnd_data = lnd_data[lnd_data['Category'] != \"Compliance\"]\n",
    "\n",
    "# move the Future Skills and Industry to Technical\n",
    "lnd_data['Category'][lnd_data['Category'] == \"Future Skills\"] = \"Technical\"\n",
    "lnd_data['Category'][lnd_data['Category'] == \"Industry\"] = \"Technical\"\n",
    "\n",
    "# extract year and quarter from the learning completion date\n",
    "lnd_data['Quarter'] = pd.PeriodIndex(lnd_data['Completion Date'], freq='Q').strftime('Q%q')\n",
    "lnd_data['Year'] = pd.PeriodIndex(lnd_data['Completion Date'], freq='Y').strftime('%Y')\n",
    "lnd_data['YearQuarter'] = lnd_data['Year'] + lnd_data['Quarter']\n",
    "lnd_data = lnd_data[['Masked Code', 'YearQuarter', 'Category','Learning Hours']]\n",
    "\n",
    "# calculate learning hours\n",
    "lnd_data_learning_hours = lnd_data.groupby(['Masked Code', 'YearQuarter', 'Category'], as_index=False).agg({'Learning Hours':'sum'})\n",
    "\n",
    "# create features by category for learning_hours\n",
    "lnd_data_learning_hours['Technical_Learning_hours'] = np.where((lnd_data_learning_hours['Category'] == 'Technical'), lnd_data_learning_hours['Learning Hours'], 0)\n",
    "lnd_data_learning_hours['ProfLeadDev_Learning_hours'] = np.where((lnd_data_learning_hours['Category'] == 'Professional & Leadership Development'), lnd_data_learning_hours['Learning Hours'], 0)\n",
    "lnd_data_learning_hours = lnd_data_learning_hours[['Masked Code', 'YearQuarter', 'Technical_Learning_hours', 'ProfLeadDev_Learning_hours']]\n",
    "lnd_data_learning_hours = lnd_data_learning_hours.groupby(['Masked Code', 'YearQuarter'], as_index=False).agg({'Technical_Learning_hours':'sum'\n",
    "                                                                                                          ,'ProfLeadDev_Learning_hours':'sum'})\n",
    "\n",
    "# calculate number of courses taken\n",
    "lnd_data_num_courses = lnd_data.groupby(['Masked Code', 'YearQuarter', 'Category'], as_index=False).count()\n",
    "lnd_data_num_courses.columns = ['Masked Code','YearQuarter','Category','NUM_COURSES']\n",
    "\n",
    "# create features by category for num_courses\n",
    "lnd_data_num_courses['Technical_num_courses'] = np.where((lnd_data_num_courses['Category'] == 'Technical'), lnd_data_num_courses['NUM_COURSES'], 0)\n",
    "lnd_data_num_courses['ProfLeadDev_num_courses'] = np.where((lnd_data_num_courses['Category'] == 'Professional & Leadership Development'), lnd_data_num_courses['NUM_COURSES'], 0)\n",
    "lnd_data_num_courses = lnd_data_num_courses[['Masked Code', 'YearQuarter', 'Technical_num_courses', 'ProfLeadDev_num_courses']]\n",
    "lnd_data_num_courses = lnd_data_num_courses.groupby(['Masked Code', 'YearQuarter'], as_index=False).agg({'Technical_num_courses':'sum' \n",
    "                                                                                                     ,'ProfLeadDev_num_courses':'sum'})\n",
    "\n",
    "\n",
    "\n",
    "# enrich with lnd data\n",
    "# learning_hours\n",
    "lnd_data_learning_hours_lastestQtr = lnd_data_learning_hours[lnd_data_learning_hours['YearQuarter'] == yr_qtr1]\n",
    "lnd_data_learning_hours_lastestQtr = lnd_data_learning_hours_lastestQtr[['Masked Code', 'Technical_Learning_hours', 'ProfLeadDev_Learning_hours']]\n",
    "lnd_data_learning_hours_lastestQtr.columns = ['Masked Code', 'LatestQtrTechLearnHours', 'LatestQtrProfLeadDevLearnHours']\n",
    "DATA = pd.merge(DATA, lnd_data_learning_hours_lastestQtr, on='Masked Code', how='left')\n",
    "\n",
    "lnd_data_learning_hours_latestYr = lnd_data_learning_hours[(lnd_data_learning_hours['YearQuarter'] == yr_qtr1) | \n",
    "                                                          (lnd_data_learning_hours['YearQuarter'] == yr_qtr2) |\n",
    "                                                          (lnd_data_learning_hours['YearQuarter'] == yr_qtr3) |\n",
    "                                                          (lnd_data_learning_hours['YearQuarter'] == yr_qtr4)]\n",
    "lnd_data_learning_hours_latestYr = lnd_data_learning_hours_latestYr[['Masked Code', 'Technical_Learning_hours', 'ProfLeadDev_Learning_hours']]\n",
    "lnd_data_learning_hours_latestYr.columns = ['Masked Code', 'LatestYrTechLearnHours', 'LatestYrProfLeadDevLearnHours']\n",
    "lnd_data_learning_hours_latestYr = lnd_data_learning_hours_latestYr.groupby(['Masked Code'], as_index=False).agg({'LatestYrTechLearnHours':'sum'\n",
    "                                                                                                                      ,'LatestYrProfLeadDevLearnHours':'sum'})\n",
    "\n",
    "DATA = pd.merge(DATA, lnd_data_learning_hours_latestYr, on='Masked Code', how='left')\n",
    "\n",
    "DATA['LatestQtrTechLearnHours'] = DATA['LatestQtrTechLearnHours'].fillna(0)\n",
    "DATA['LatestQtrProfLeadDevLearnHours'] = DATA['LatestQtrProfLeadDevLearnHours'].fillna(0)\n",
    "DATA['LatestYrTechLearnHours'] = DATA['LatestYrTechLearnHours'].fillna(0)\n",
    "DATA['LatestYrProfLeadDevLearnHours'] = DATA['LatestYrProfLeadDevLearnHours'].fillna(0)\n",
    "\n",
    "# number of courses\n",
    "lnd_data_num_courses_lastestQtr = lnd_data_num_courses[lnd_data_num_courses['YearQuarter'] == yr_qtr1]\n",
    "lnd_data_num_courses_lastestQtr = lnd_data_num_courses_lastestQtr[['Masked Code', 'Technical_num_courses', 'ProfLeadDev_num_courses']]\n",
    "lnd_data_num_courses_lastestQtr.columns = ['Masked Code', 'LatestQtrTechNumCourses', 'LatestQtrProfLeadDevNumCourses']\n",
    "DATA = pd.merge(DATA, lnd_data_num_courses_lastestQtr, on='Masked Code', how='left')\n",
    "\n",
    "lnd_data_num_courses_latestYr = lnd_data_num_courses[(lnd_data_num_courses['YearQuarter'] == yr_qtr1) | \n",
    "                                                          (lnd_data_num_courses['YearQuarter'] == yr_qtr2) |\n",
    "                                                          (lnd_data_num_courses['YearQuarter'] == yr_qtr3) |\n",
    "                                                          (lnd_data_num_courses['YearQuarter'] == yr_qtr4)]\n",
    "lnd_data_num_courses_latestYr = lnd_data_num_courses_latestYr[['Masked Code', 'Technical_num_courses', 'ProfLeadDev_num_courses']]\n",
    "lnd_data_num_courses_latestYr.columns = ['Masked Code', 'LatestYrTechNumCourses', 'LatestYrProfLeadDevNumCourses']\n",
    "lnd_data_num_courses_latestYr = lnd_data_num_courses_latestYr.groupby(['Masked Code'], as_index=False).agg({'LatestYrTechNumCourses':'sum'\n",
    "                                                                                                                      ,'LatestYrProfLeadDevNumCourses':'sum'})\n",
    "\n",
    "DATA = pd.merge(DATA, lnd_data_num_courses_latestYr, on='Masked Code', how='left')\n",
    "\n",
    "DATA['LatestQtrTechNumCourses'] = DATA['LatestQtrTechNumCourses'].fillna(0)\n",
    "DATA['LatestQtrProfLeadDevNumCourses'] = DATA['LatestQtrProfLeadDevNumCourses'].fillna(0)\n",
    "DATA['LatestYrTechNumCourses'] = DATA['LatestYrTechNumCourses'].fillna(0)\n",
    "DATA['LatestYrProfLeadDevNumCourses'] = DATA['LatestYrProfLeadDevNumCourses'].fillna(0)\n",
    "\n",
    "print(\"Successfully executed LnD Data\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\RnR Data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed RnR Data\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rnr_data = pd.read_excel('RnR Data.xlsx', sheet_name = \"Sheet1\")\n",
    "\n",
    "\n",
    "rnr_data['CompletionDate'] = pd.to_datetime(rnr_data['Nomination Date'])\n",
    "rnr_data = rnr_data[['Masked Code', 'Type', 'Reward name', 'Nomination Date', 'Amount(Rs)']]\n",
    "\n",
    "# keep the records of only employees having rewards and recognitions\n",
    "rnr_data = rnr_data[(rnr_data['Type'] == 'Reward') | (rnr_data['Type'] == 'Recognition')]\n",
    "\n",
    "# extract year and quarter from the Nomination Date\n",
    "rnr_data['Quarter'] = pd.PeriodIndex(rnr_data['Nomination Date'], freq='Q').strftime('Q%q')\n",
    "rnr_data['Year'] = pd.PeriodIndex(rnr_data['Nomination Date'], freq='Y').strftime('%Y')\n",
    "rnr_data['YearQuarter'] = rnr_data['Year'] + rnr_data['Quarter']\n",
    "rnr_data = rnr_data[['Masked Code', 'YearQuarter', 'Type','Reward name', 'Amount(Rs)']]\n",
    "\n",
    "# calculate amount received from rewards/recognitions (look for bins later)\n",
    "rnr_amount_data = rnr_data.groupby(['Masked Code', 'YearQuarter'], as_index=False).agg({'Amount(Rs)':'sum'})\n",
    "\n",
    "# calculate count of rewards/recognitions\n",
    "rnr_data = rnr_data[['Masked Code', 'YearQuarter', 'Type', 'Reward name']]\n",
    "rnr_data = rnr_data.groupby(['Masked Code', 'YearQuarter', 'Type'], as_index=False).count()\n",
    "rnr_data.columns = ['Masked Code', 'YearQuarter', 'Type', 'Count']\n",
    "\n",
    "rnr_data['NumOfRewards'] = np.where((rnr_data['Type'] == 'Reward'), rnr_data['Count'], 0)\n",
    "rnr_data['NumOfRecognitions'] = np.where((rnr_data['Type'] == 'Recognition'), rnr_data['Count'], 0)\n",
    "rnr_data = rnr_data.groupby(['Masked Code', 'YearQuarter'], as_index=False).agg({'NumOfRewards':'sum', 'NumOfRecognitions':'sum'})\n",
    "\n",
    "# calculate total RnR\n",
    "rnr_data['TotalRnR'] = rnr_data['NumOfRewards'] + rnr_data['NumOfRecognitions']\n",
    "\n",
    "# derive the columns Is Recognized and Is Rewarded\n",
    "rnr_data['IsRewarded'] = np.where((rnr_data['NumOfRewards'] >= 1), 1, 0)\n",
    "rnr_data['IsRecognized'] = np.where((rnr_data['NumOfRecognitions'] >= 1), 1, 0)\n",
    "\n",
    "# enrich with RnR data\n",
    "# RnR awards\n",
    "rnr_data = rnr_data[rnr_data['YearQuarter'] == yr_qtr1]\n",
    "rnr_data = rnr_data[['Masked Code', 'NumOfRewards', 'NumOfRecognitions', 'TotalRnR', 'IsRewarded', 'IsRecognized']]\n",
    "DATA = pd.merge(DATA, rnr_data, on='Masked Code', how='left')\n",
    "\n",
    "# RnR amount\n",
    "rnr_amount_data = rnr_amount_data[rnr_amount_data['YearQuarter'] == yr_qtr1]\n",
    "rnr_amount_data = rnr_amount_data[['Masked Code', 'Amount(Rs)']]\n",
    "DATA = pd.merge(DATA, rnr_amount_data, on='Masked Code', how='left')\n",
    "\n",
    "# fill missing values with 0\n",
    "DATA['NumOfRewards'] = DATA['NumOfRewards'].fillna(0)\n",
    "DATA['NumOfRecognitions'] = DATA['NumOfRecognitions'].fillna(0)\n",
    "DATA['TotalRnR'] = DATA['TotalRnR'].fillna(0)\n",
    "DATA['IsRewarded'] = DATA['IsRewarded'].fillna(0)\n",
    "DATA['IsRecognized'] = DATA['IsRecognized'].fillna(0)\n",
    "DATA['Amount(Rs)'] = DATA['Amount(Rs)'].fillna(0)\n",
    "\n",
    "print(\"Successfully executed RnR Data\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\Snapshot Data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed Snapshot Results Data\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "snapshot_results_data = pd.read_excel('Performance Snapshot Data.xlsx' , sheet_name ='Sheet1')\n",
    "\n",
    "\n",
    "# subset required columns\n",
    "snapshot_results_data = snapshot_results_data[['Masked Emp. Code.', 'Created Date','Q1 Answer Value','Q2 Answer Value','Q3 Answer Value', 'Q4 Answer Value']]\n",
    "snapshot_results_data.columns = ['Masked Code','Date Created' ,'Q1_ans','Q2_ans','Q3_ans','Q4_ans']\n",
    "\n",
    "snapshot_results_data['Date Created'] = pd.to_datetime(snapshot_results_data['Date Created'])\n",
    "\n",
    "# extract year and quarter from the Snapshot Creation Date\n",
    "snapshot_results_data['Quarter'] = pd.PeriodIndex(snapshot_results_data['Date Created'], freq='Q').strftime('Q%q')\n",
    "snapshot_results_data['Year'] = pd.PeriodIndex(snapshot_results_data['Date Created'], freq='Y').strftime('%Y')\n",
    "snapshot_results_data['YearQuarter'] = snapshot_results_data['Year'] + snapshot_results_data['Quarter']\n",
    "snapshot_results_data = snapshot_results_data[['Masked Code', 'YearQuarter','Q1_ans','Q2_ans','Q3_ans','Q4_ans']]\n",
    "\n",
    "snapshot_results_data['Q1_ans_weightage'] = np.where((snapshot_results_data['Q1_ans'] == 5), 100,\n",
    "                                                 np.where((snapshot_results_data['Q1_ans'] == 4), 80,\n",
    "                                                          np.where((snapshot_results_data['Q1_ans'] == 3), 60,\n",
    "                                                                   np.where((snapshot_results_data['Q1_ans'] == 2), 40,\n",
    "                                                                            np.where((snapshot_results_data['Q1_ans'] == 1), 20,\n",
    "                                                                            0)))))\n",
    "\n",
    "snapshot_results_data['Q2_ans_weightage'] = np.where((snapshot_results_data['Q2_ans'] == 5), 100,\n",
    "                                                 np.where((snapshot_results_data['Q2_ans'] == 4), 80,\n",
    "                                                          np.where((snapshot_results_data['Q2_ans'] == 3), 60,\n",
    "                                                                   np.where((snapshot_results_data['Q2_ans'] == 2), 40,\n",
    "                                                                            np.where((snapshot_results_data['Q2_ans'] == 1), 20,\n",
    "                                                                            0)))))\n",
    "\n",
    "snapshot_results_data['Q3_ans_weightage'] = np.where((snapshot_results_data['Q3_ans'] == 2), 100,0)\n",
    "\n",
    "snapshot_results_data['Q4_ans_weightage'] = np.where((snapshot_results_data['Q4_ans'] == 1), 100,0)\n",
    "\n",
    "snapshot_results_data['snapshot_composite_score'] = 0.25* (snapshot_results_data['Q1_ans_weightage'] + snapshot_results_data['Q2_ans_weightage']+ snapshot_results_data['Q3_ans_weightage']+snapshot_results_data['Q4_ans_weightage'])\n",
    "\n",
    "snapshot_results_data = snapshot_results_data[['Masked Code', 'YearQuarter', 'snapshot_composite_score']]\n",
    "\n",
    "\n",
    "# import Firm Contribution Snapshot Results data\n",
    "snapshot_fi_results_data = pd.read_excel('Firm Contribution Snapshot Data.xlsx' , sheet_name ='Sheet1')\n",
    "\n",
    "\n",
    "\n",
    "# subset required columns\n",
    "snapshot_fi_results_data = snapshot_fi_results_data[['Masked emp. Code.', 'Created Date','Q1 Answer Value','Q2 Answer Value']]\n",
    "snapshot_fi_results_data.columns = ['Masked Code','Date Created' ,'Q1_ans','Q2_ans']\n",
    "\n",
    "snapshot_fi_results_data['Date Created'] = pd.to_datetime(snapshot_fi_results_data['Date Created'])\n",
    "\n",
    "# extract year and quarter from the Snapshot Creation Date\n",
    "snapshot_fi_results_data['Quarter'] = pd.PeriodIndex(snapshot_fi_results_data['Date Created'], freq='Q').strftime('Q%q')\n",
    "snapshot_fi_results_data['Year'] = pd.PeriodIndex(snapshot_fi_results_data['Date Created'], freq='Y').strftime('%Y')\n",
    "snapshot_fi_results_data['YearQuarter'] = snapshot_fi_results_data['Year'] + snapshot_fi_results_data['Quarter']\n",
    "snapshot_fi_results_data = snapshot_fi_results_data[['Masked Code', 'YearQuarter','Q1_ans','Q2_ans']]\n",
    "\n",
    "snapshot_fi_results_data['Q1_ans_weightage'] = np.where((snapshot_fi_results_data['Q1_ans'] == 5), 100,\n",
    "                                                 np.where((snapshot_fi_results_data['Q1_ans'] == 4), 80,\n",
    "                                                          np.where((snapshot_fi_results_data['Q1_ans'] == 3), 60,\n",
    "                                                                   np.where((snapshot_fi_results_data['Q1_ans'] == 2), 40,\n",
    "                                                                            np.where((snapshot_fi_results_data['Q1_ans'] == 1), 20,\n",
    "                                                                            0)))))\n",
    "\n",
    "snapshot_fi_results_data['Q2_ans_weightage'] = np.where((snapshot_fi_results_data['Q2_ans'] == 5), 100,\n",
    "                                                 np.where((snapshot_fi_results_data['Q2_ans'] == 4), 80,\n",
    "                                                          np.where((snapshot_fi_results_data['Q2_ans'] == 3), 60,\n",
    "                                                                   np.where((snapshot_fi_results_data['Q2_ans'] == 2), 40,\n",
    "                                                                            np.where((snapshot_fi_results_data['Q2_ans'] == 1), 20,\n",
    "                                                                            0)))))\n",
    "\n",
    "snapshot_fi_results_data['snapshot_composite_score'] = 0.5* (snapshot_fi_results_data['Q1_ans_weightage'] + snapshot_fi_results_data['Q2_ans_weightage'])\n",
    "\n",
    "snapshot_fi_results_data = snapshot_fi_results_data[['Masked Code', 'YearQuarter', 'snapshot_composite_score']]\n",
    "\n",
    "# append performance and firm contribution snapshot results\n",
    "snapshot_results_data = pd.concat([snapshot_results_data, snapshot_fi_results_data], axis=0)\n",
    "\n",
    "# normalising data\n",
    "snapshot_results_data['snapshot_composite_score'] = (snapshot_results_data['snapshot_composite_score'])/100\n",
    "\n",
    "# enrich with the snapshot results data\n",
    "# by latest quarter\n",
    "snapshot_results_data_latestQtr = snapshot_results_data[snapshot_results_data['YearQuarter'] == yr_qtr1]\n",
    "snapshot_results_data_latestQtr = snapshot_results_data_latestQtr[['Masked Code', 'snapshot_composite_score']]\n",
    "snapshot_results_data_latestQtr.columns = ['Masked Code','LatestQtr_snapshot_composite_score']\n",
    "snapshot_results_data_latestQtr = snapshot_results_data_latestQtr.groupby(['Masked Code'], as_index=False).agg({'LatestQtr_snapshot_composite_score':'mean'})\n",
    "DATA = pd.merge(DATA, snapshot_results_data_latestQtr, on='Masked Code', how='left')\n",
    "\n",
    "# by latest year\n",
    "# filter the latest year\n",
    "snapshot_results_data_latestYr = snapshot_results_data[(snapshot_results_data['YearQuarter'] == yr_qtr1) | \n",
    "                                                      (snapshot_results_data['YearQuarter'] == yr_qtr2) |\n",
    "                                                      (snapshot_results_data['YearQuarter'] == yr_qtr3) |\n",
    "                                                      (snapshot_results_data['YearQuarter'] == yr_qtr4)]\n",
    "\n",
    "snapshot_results_data_latestYr = snapshot_results_data_latestYr[['Masked Code', 'snapshot_composite_score']]\n",
    "snapshot_results_data_latestYr.columns = ['Masked Code','LatestYr_snapshot_composite_score']\n",
    "snapshot_results_data_latestYr = snapshot_results_data_latestYr.groupby(['Masked Code'], as_index=False).agg({'LatestYr_snapshot_composite_score':'mean'})\n",
    "DATA= pd.merge(DATA, snapshot_results_data_latestYr, on='Masked Code', how='left')\n",
    "\n",
    "print(\"Successfully executed Snapshot Results Data\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2020Q3', '2020Q2', '2020Q4', '2020Q1', '2019Q2', '2019Q4',\n",
       "       '2019Q3', '2019Q1', '2018Q2', '2018Q4', '2018Q3', '2018Q1',\n",
       "       '2017Q1', '2017Q2', '2017Q4', '2017Q3', '2016Q4', '2016Q3'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_results_data['YearQuarter'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed Snapshot Data\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import snapshot data\n",
    "snapshot_perf_data = pd.read_excel('Performance Snapshot Data.xlsx' , sheet_name ='Sheet1')\n",
    "snapshot_perf_data = snapshot_perf_data[['Masked Emp. Code.', 'Created Date', 'Project Start Date', 'Project End Date', 'Status', 'Snapshot Type', 'Project Hours  ']]\n",
    "snapshot_perf_data.columns = ['Masked Code','Date Created','Project Start Date', 'Project End Date', 'Status', 'Snapshot Name','Project Hours']\n",
    "\n",
    "snapshot_firm_data = pd.read_excel('Firm Contribution Snapshot Data.xlsx' , sheet_name ='Sheet1')\n",
    "snapshot_firm_data = snapshot_firm_data[['Masked emp. Code.', 'Created Date', 'Project Start Date', 'Project End Date', 'Status', 'Snapshot Type', 'Project Hours  ']]\n",
    "snapshot_firm_data.columns = ['Masked Code','Date Created','Project Start Date', 'Project End Date', 'Status', 'Snapshot Name','Project Hours']\n",
    "\n",
    "# append performance and firm contribution snapshots\n",
    "snapshot_data = pd.concat([snapshot_perf_data, snapshot_firm_data], axis=0)\n",
    "\n",
    "snapshot_project_data = snapshot_data[['Masked Code', 'Project Start Date', 'Project End Date']]\n",
    "snapshot_data = snapshot_data[['Masked Code', 'Date Created', 'Status', 'Snapshot Name', 'Project Hours']]\n",
    "\n",
    "# renaming the columns\n",
    "snapshot_project_data.columns = ['Masked Code', 'Project Start Date', 'Project End Date']\n",
    "\n",
    "# format the dates\n",
    "snapshot_project_data['Project Start Date'] = pd.to_datetime(snapshot_project_data['Project Start Date'])\n",
    "snapshot_project_data['Project End Date'] = pd.to_datetime(snapshot_project_data['Project End Date'])\n",
    "\n",
    "if min(((snapshot_project_data['Project End Date'] - snapshot_project_data['Project Start Date'])/\n",
    "                                                                  np.timedelta64(1, 'D')).astype(int)):\n",
    "    print(\"Data Quality Issue - Snapshot Data has negative project days\\r\\n\")\n",
    "\n",
    "\n",
    "    # logic to calculate the project days by quarter and by latest year\n",
    "snapshot_project_data['QEQSDiff_QB4'] = (q4_end - q4_start) + timedelta(days=1)\n",
    "snapshot_project_data['PEQSDiff_QB4'] = (snapshot_project_data['Project End Date']  - q4_start) + timedelta(days=1)\n",
    "snapshot_project_data['QEPSDiff_QB4'] = (q4_end - snapshot_project_data['Project Start Date']) + timedelta(days=1)\n",
    "snapshot_project_data['PEPSDiff_QB4'] = (snapshot_project_data['Project End Date']  - snapshot_project_data['Project Start Date']) + timedelta(days=1)\n",
    "\n",
    "snapshot_project_data['QEQSDiff_QB3'] = (q3_end - q3_start) + timedelta(days=1)\n",
    "snapshot_project_data['PEQSDiff_QB3'] = (snapshot_project_data['Project End Date']  - q3_start) + timedelta(days=1)\n",
    "snapshot_project_data['QEPSDiff_QB3'] = (q3_end - snapshot_project_data['Project Start Date']) + timedelta(days=1)\n",
    "snapshot_project_data['PEPSDiff_QB3'] = (snapshot_project_data['Project End Date']  - snapshot_project_data['Project Start Date']) + timedelta(days=1)\n",
    "\n",
    "snapshot_project_data['QEQSDiff_QB2'] = (q2_end - q2_start) + timedelta(days=1)\n",
    "snapshot_project_data['PEQSDiff_QB2'] = (snapshot_project_data['Project End Date']  - q2_start) + timedelta(days=1)\n",
    "snapshot_project_data['QEPSDiff_QB2'] = (q2_end - snapshot_project_data['Project Start Date']) + timedelta(days=1)\n",
    "snapshot_project_data['PEPSDiff_QB2'] = (snapshot_project_data['Project End Date']  - snapshot_project_data['Project Start Date']) + timedelta(days=1)\n",
    "\n",
    "snapshot_project_data['QEQSDiff_QB1'] = (q1_end - q1_start) + timedelta(days=1)\n",
    "snapshot_project_data['PEQSDiff_QB1'] = (snapshot_project_data['Project End Date']  - q1_start) + timedelta(days=1)\n",
    "snapshot_project_data['QEPSDiff_QB1'] = (q1_end - snapshot_project_data['Project Start Date']) + timedelta(days=1)\n",
    "snapshot_project_data['PEPSDiff_QB1'] = (snapshot_project_data['Project End Date']  - snapshot_project_data['Project Start Date']) + timedelta(days=1)\n",
    "\n",
    "# calculate the number of days an employee has worked in each quarter\n",
    "# work on 4th quarter\n",
    "cols_project_QB4 = ['Project Start Date', 'Project End Date', 'QEQSDiff_QB4',  'PEQSDiff_QB4', 'QEPSDiff_QB4', 'PEPSDiff_QB4']\n",
    "snapshot_project_data[yr_qtr4] = quarter_days_allocator(snapshot_project_data, cols_project_QB4, q4_start, q4_end)\n",
    "\n",
    "# work on 3rd quarter\n",
    "cols_project_QB3 = ['Project Start Date', 'Project End Date', 'QEQSDiff_QB3',  'PEQSDiff_QB3', 'QEPSDiff_QB3', 'PEPSDiff_QB3']\n",
    "snapshot_project_data[yr_qtr3] = quarter_days_allocator(snapshot_project_data, cols_project_QB3, q3_start, q3_end)\n",
    "\n",
    "# work on 2nd quarter\n",
    "cols_project_QB2 = ['Project Start Date', 'Project End Date', 'QEQSDiff_QB2',  'PEQSDiff_QB2', 'QEPSDiff_QB2', 'PEPSDiff_QB2']\n",
    "snapshot_project_data[yr_qtr2] = quarter_days_allocator(snapshot_project_data, cols_project_QB2, q2_start, q2_end)\n",
    "\n",
    "# work on 1st quarter\n",
    "cols_project_QB1 = ['Project Start Date', 'Project End Date', 'QEQSDiff_QB1',  'PEQSDiff_QB1', 'QEPSDiff_QB1', 'PEPSDiff_QB1']\n",
    "snapshot_project_data[yr_qtr1] = quarter_days_allocator(snapshot_project_data, cols_project_QB1, q1_start, q1_end)\n",
    "\n",
    "\n",
    "\n",
    "# subset only required columns\n",
    "snapshot_project_data = snapshot_project_data[['Masked Code',yr_qtr4, yr_qtr3, yr_qtr2, yr_qtr1]]\n",
    "\n",
    "# converting from days to int\n",
    "snapshot_project_data[yr_qtr4] = (snapshot_project_data[yr_qtr4] / np.timedelta64(1, 'D')).astype(int)\n",
    "snapshot_project_data[yr_qtr3] = (snapshot_project_data[yr_qtr3] / np.timedelta64(1, 'D')).astype(int)\n",
    "snapshot_project_data[yr_qtr2] = (snapshot_project_data[yr_qtr2] / np.timedelta64(1, 'D')).astype(int)\n",
    "snapshot_project_data[yr_qtr1] = (snapshot_project_data[yr_qtr1] / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "# grouping by employees\n",
    "snapshot_project_data = snapshot_project_data.groupby(['Masked Code'], as_index=False).agg({yr_qtr4:'sum'\n",
    "                                                                                      ,yr_qtr3:'sum'                                                                           \n",
    "                                                                                      ,yr_qtr2:'sum'\n",
    "                                                                                      ,yr_qtr1:'sum'})\n",
    "\n",
    "# unpivot the dataset\n",
    "snapshot_project_data = snapshot_project_data.melt(id_vars=['Masked Code'], var_name='YearQuarter', value_name='ProjectDays')\n",
    "\n",
    "# logic to calculate the count of snapshots\n",
    "# format the dates\n",
    "snapshot_data['Date Created'] = pd.to_datetime(snapshot_data['Date Created'])\n",
    "\n",
    "# calculate the features on number of snapshots\n",
    "# extract year and quarter from the Snapshot Creation Date\n",
    "snapshot_data['Quarter'] = pd.PeriodIndex(snapshot_data['Date Created'], freq='Q').strftime('Q%q')\n",
    "snapshot_data['Year'] = pd.PeriodIndex(snapshot_data['Date Created'], freq='Y').strftime('%Y')\n",
    "snapshot_data['YearQuarter'] = snapshot_data['Year'] + snapshot_data['Quarter']\n",
    "snapshot_data = snapshot_data[['Masked Code', 'YearQuarter','Status', 'Snapshot Name','Project Hours']]\n",
    "\n",
    "# calculate number of performance snapshots and other snapshots\n",
    "snapshot_data['PerfSnapshot'] = np.where((snapshot_data['Snapshot Name'] == 'Performance Snapshot'), 1, 0)\n",
    "snapshot_data['OtherSnapshot'] = np.where((snapshot_data['Snapshot Name'] == 'Firm Contribution Snapshot'), 1, 0)\n",
    "snapshot_data = snapshot_data.groupby(['Masked Code', 'YearQuarter', 'Status'], as_index=False).agg({'PerfSnapshot':'sum', 'OtherSnapshot':'sum'})\n",
    "\n",
    "# cap the snapshot counts\n",
    "snapshot_data['PerfSnapshot'] = np.where((snapshot_data['PerfSnapshot'] > 15), 15, snapshot_data['PerfSnapshot'])\n",
    "\n",
    "# calculate total number of snapshots\n",
    "snapshot_data['TotalSnapshot'] = snapshot_data['PerfSnapshot'] + snapshot_data['OtherSnapshot']\n",
    "\n",
    "\n",
    "# enrich with the snapshot data\n",
    "# filter and merge the snapshot project days data\n",
    "# by latest quarter\n",
    "snapshot_project_data_latestQtr = snapshot_project_data[snapshot_project_data['YearQuarter'] == yr_qtr1]\n",
    "snapshot_project_data_latestQtr = snapshot_project_data_latestQtr[['Masked Code', 'ProjectDays']]\n",
    "snapshot_project_data_latestQtr.columns = ['Masked Code','LatestQtrProjectDays']\n",
    "DATA = pd.merge(DATA, snapshot_project_data_latestQtr, on='Masked Code', how='left')\n",
    "\n",
    "# by latest year\n",
    "snapshot_project_data_latestYr = snapshot_project_data[(snapshot_project_data['YearQuarter'] == yr_qtr1) | \n",
    "                                                      (snapshot_project_data['YearQuarter'] == yr_qtr2) |\n",
    "                                                      (snapshot_project_data['YearQuarter'] == yr_qtr3) |\n",
    "                                                      (snapshot_project_data['YearQuarter'] == yr_qtr4)]\n",
    "snapshot_project_data_latestYr = snapshot_project_data_latestYr[['Masked Code', 'ProjectDays']]\n",
    "snapshot_project_data_latestYr.columns = ['Masked Code','LatestYrProjectDays']\n",
    "snapshot_project_data_latestYr = snapshot_project_data_latestYr.groupby(['Masked Code'], as_index=False).agg({'LatestYrProjectDays':'sum'})\n",
    "DATA = pd.merge(DATA, snapshot_project_data_latestYr, on='Masked Code', how='left')\n",
    "\n",
    "# filter and merge the snapshot count data\n",
    "# by latest quarter\n",
    "snapshot_data_latestQtr = snapshot_data[snapshot_data['YearQuarter'] == yr_qtr1]\n",
    "snapshot_data_latestQtr = snapshot_data_latestQtr[['Masked Code', 'PerfSnapshot', 'OtherSnapshot', 'TotalSnapshot']]\n",
    "\n",
    "snapshot_data_latestQtr.columns = ['Masked Code', 'LatestQtrPerfSnapshot', 'LatestQtrOtherSnapshot', 'LatestQtrTotalSnapshot']\n",
    "snapshot_data_latestQtr = snapshot_data_latestQtr.groupby(['Masked Code'], as_index=False).agg({'LatestQtrPerfSnapshot':'sum'\n",
    "                                                                                                      ,'LatestQtrOtherSnapshot':'sum'\n",
    "                                                                                                      ,'LatestQtrTotalSnapshot':'sum'})\n",
    "\n",
    "DATA = pd.merge(DATA, snapshot_data_latestQtr, on='Masked Code', how='left')\n",
    "\n",
    "# by latest year\n",
    "snapshot_data_latestYr = snapshot_data[(snapshot_data['YearQuarter'] == yr_qtr1) | \n",
    "                                          (snapshot_data['YearQuarter'] == yr_qtr2) |\n",
    "                                          (snapshot_data['YearQuarter'] == yr_qtr3) |\n",
    "                                          (snapshot_data['YearQuarter'] == yr_qtr4)]\n",
    "snapshot_data_latestYr = snapshot_data_latestYr[['Masked Code', 'PerfSnapshot', 'OtherSnapshot', 'TotalSnapshot']]\n",
    "snapshot_data_latestYr.columns = ['Masked Code', 'LatestYrPerfSnapshot', 'LatestYrOtherSnapshot', 'LatestYrTotalSnapshot']\n",
    "snapshot_data_latestYr = snapshot_data_latestYr.groupby(['Masked Code'], as_index=False).agg({'LatestYrPerfSnapshot':'sum'\n",
    "                                                                                                    ,'LatestYrOtherSnapshot':'sum'\n",
    "                                                                                                    ,'LatestYrTotalSnapshot':'sum'})\n",
    "\n",
    "DATA = pd.merge(DATA, snapshot_data_latestYr, on='Masked Code', how='left')\n",
    "\n",
    "# fill the missing values with 0\n",
    "DATA['LatestQtrPerfSnapshot'] = DATA['LatestQtrPerfSnapshot'].fillna(0)\n",
    "DATA['LatestQtrOtherSnapshot'] = DATA['LatestQtrOtherSnapshot'].fillna(0)\n",
    "DATA['LatestQtrTotalSnapshot'] = DATA['LatestQtrTotalSnapshot'].fillna(0)\n",
    "DATA['LatestYrPerfSnapshot'] = DATA['LatestYrPerfSnapshot'].fillna(0)\n",
    "DATA['LatestYrOtherSnapshot'] = DATA['LatestYrOtherSnapshot'].fillna(0)\n",
    "DATA['LatestYrTotalSnapshot'] = DATA['LatestYrTotalSnapshot'].fillna(0)\n",
    "\n",
    "print(\"Successfully executed Snapshot Data\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\Leave Ledger Data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed Leave Ledger Data\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "leave_data_active = pd.read_excel('Leave Ledger for Active.xlsx' , sheet_name ='Sheet1')\n",
    "leave_data_inactive = pd.read_excel('Leave Ledger for Inactive.xlsx' , sheet_name ='Sheet1')\n",
    "\n",
    "\n",
    "\n",
    "# subset required columns\n",
    "leave_data_active = leave_data_active[['Masked Code', 'Leave Type','From Date','To Date', 'Total No. Of Leave Days / Hours','Status']]\n",
    "leave_data_active.columns = ['Masked Code', 'Leave Type','From Date','To Date','TotalLeaveDays','Status']\n",
    "\n",
    "leave_data_inactive = leave_data_inactive[['Masked Code', 'Leave Type','From Date','To Date','Total No. Of Leave Days / Hours','Status']]\n",
    "leave_data_inactive.columns = ['Masked Code', 'Leave Type','From Date','To Date','TotalLeaveDays','Status']\n",
    "\n",
    "# append the 2 datasets\n",
    "leave_data = pd.concat([leave_data_active, leave_data_inactive], axis=0)\n",
    "\n",
    "# format the date\n",
    "leave_data['From Date'] = pd.to_datetime(leave_data['From Date'])\n",
    "leave_data['To Date'] = pd.to_datetime(leave_data['To Date'])\n",
    "\n",
    "# filter leaves requests for leave utilization calculation\n",
    "leave_data_request = leave_data[(leave_data['Leave Type']== 'LE-Request') | \n",
    "                            (leave_data['Leave Type']== 'SPL-Request')|\n",
    "                            (leave_data['Leave Type']== 'HTO-Request')|\n",
    "                            (leave_data['Leave Type']== 'LWP-Request')|\n",
    "                            (leave_data['Leave Type']== 'ML-Request') |\n",
    "                            (leave_data['Leave Type']== 'PL-Request') |\n",
    "                            (leave_data['Leave Type']== 'BL-Request') |\n",
    "                            (leave_data['Leave Type']== 'RTL-Request')]\n",
    "\n",
    "#  separate approved and rejected leaves data \n",
    "leave_approved_data = leave_data_request[leave_data_request['Status']== 'Approved']\n",
    "leave_declined_data = leave_data_request[leave_data_request['Status']== 'Rejected']\n",
    "\n",
    "# calculate the count of rejected leaves\n",
    "leave_declined_data = leave_declined_data[['Masked Code','From Date']]\n",
    "\n",
    "# extract year and quarter from the leave request from date\n",
    "leave_declined_data['Quarter'] = pd.PeriodIndex(leave_declined_data['From Date'], freq='Q').strftime('Q%q')\n",
    "leave_declined_data['Year'] = pd.PeriodIndex(leave_declined_data['From Date'], freq='Y').strftime('%Y')\n",
    "leave_declined_data['YearQuarter'] = leave_declined_data['Year'] + leave_declined_data['Quarter']\n",
    "\n",
    "leave_declined_data = leave_declined_data[['Masked Code', 'YearQuarter']]\n",
    "\n",
    "# count the declined leave requests\n",
    "leave_declined_data['CountOfRejections'] = 1\n",
    "leave_declined_data = leave_declined_data.groupby(['Masked Code', 'YearQuarter'], as_index=False).count()\n",
    "\n",
    "\n",
    "\n",
    "leave_approved_data['LQEQSDiff_QB4'] = (q4_end - q4_start) + timedelta(days=1)\n",
    "leave_approved_data['LEQSDiff_QB4'] = (leave_approved_data['To Date']  - q4_start) + timedelta(days=1)\n",
    "leave_approved_data['QELSDiff_QB4'] = (q4_end - leave_approved_data['From Date']) + timedelta(days=1)\n",
    "leave_approved_data['LELSDiff_QB4'] = (leave_approved_data['To Date']  - leave_approved_data['From Date']) + timedelta(days=1)\n",
    "\n",
    "leave_approved_data['LQEQSDiff_QB3'] = (q3_end - q3_start) + timedelta(days=1)\n",
    "leave_approved_data['LEQSDiff_QB3'] = (leave_approved_data['To Date']  - q3_start) + timedelta(days=1)\n",
    "leave_approved_data['QELSDiff_QB3'] = (q3_end - leave_approved_data['From Date']) + timedelta(days=1)\n",
    "leave_approved_data['LELSDiff_QB3'] = (leave_approved_data['To Date']  - leave_approved_data['From Date']) + timedelta(days=1)\n",
    "\n",
    "leave_approved_data['LQEQSDiff_QB2'] = (q2_end - q2_start) + timedelta(days=1)\n",
    "leave_approved_data['LEQSDiff_QB2'] = (leave_approved_data['To Date']  - q2_start) + timedelta(days=1)\n",
    "leave_approved_data['QELSDiff_QB2'] = (q2_end - leave_approved_data['From Date']) + timedelta(days=1)\n",
    "leave_approved_data['LELSDiff_QB2'] = (leave_approved_data['To Date']  - leave_approved_data['From Date']) + timedelta(days=1)\n",
    "\n",
    "leave_approved_data['LQEQSDiff_QB1'] = (q1_end - q1_start) + timedelta(days=1)\n",
    "leave_approved_data['LEQSDiff_QB1'] = (leave_approved_data['To Date']  - q1_start) + timedelta(days=1)\n",
    "leave_approved_data['QELSDiff_QB1'] = (q1_end - leave_approved_data['From Date']) + timedelta(days=1)\n",
    "leave_approved_data['LELSDiff_QB1'] = (leave_approved_data['To Date']  - leave_approved_data['From Date']) + timedelta(days=1)\n",
    "\n",
    "# calculate the number of days an employee has utilized leaves in each quarter\n",
    "# leave on 1st quarter\n",
    "cols_leave_QB4 = ['From Date', 'To Date', 'LQEQSDiff_QB4',  'LEQSDiff_QB4', 'QELSDiff_QB4', 'LELSDiff_QB4']\n",
    "leave_approved_data[yr_qtr4] = quarter_days_allocator(leave_approved_data, cols_leave_QB4, q4_start, q4_end)\n",
    "\n",
    "# leave on 2nd quarter 2019\n",
    "cols_leave_QB3 = ['From Date', 'To Date', 'LQEQSDiff_QB3',  'LEQSDiff_QB3', 'QELSDiff_QB3', 'LELSDiff_QB3']\n",
    "leave_approved_data[yr_qtr3] = quarter_days_allocator(leave_approved_data, cols_leave_QB3, q3_start, q3_end)\n",
    "\n",
    "# leave on 3rd quarter 2019\n",
    "cols_leave_QB2 = ['From Date', 'To Date', 'LQEQSDiff_QB2',  'LEQSDiff_QB2', 'QELSDiff_QB2', 'LELSDiff_QB2']\n",
    "leave_approved_data[yr_qtr2] = quarter_days_allocator(leave_approved_data, cols_leave_QB2, q2_start, q2_end)\n",
    "\n",
    "# leave on 4th quarter 2019\n",
    "cols_leave_QB1 = ['From Date', 'To Date', 'LQEQSDiff_QB1',  'LEQSDiff_QB1', 'QELSDiff_QB1', 'LELSDiff_QB1']\n",
    "leave_approved_data[yr_qtr1] = quarter_days_allocator(leave_approved_data, cols_leave_QB1, q1_start, q1_end)\n",
    "\n",
    "\n",
    "\n",
    "# selecting required columns\n",
    "leave_approved_data = leave_approved_data[['Masked Code', 'Leave Type', 'TotalLeaveDays',yr_qtr4,yr_qtr3,yr_qtr2,yr_qtr1]]\n",
    "\n",
    "# converting from days to int\n",
    "leave_approved_data[yr_qtr4] = (leave_approved_data[yr_qtr4] / np.timedelta64(1, 'D')).astype(int)\n",
    "leave_approved_data[yr_qtr3] = (leave_approved_data[yr_qtr3] / np.timedelta64(1, 'D')).astype(int)\n",
    "leave_approved_data[yr_qtr2] = (leave_approved_data[yr_qtr2] / np.timedelta64(1, 'D')).astype(int)\n",
    "leave_approved_data[yr_qtr1] = (leave_approved_data[yr_qtr1] / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "# grouping by employees\n",
    "leave_approved_data = leave_approved_data.groupby(['Masked Code','Leave Type'], as_index=False).agg({yr_qtr4:'sum'\n",
    "                                                                                                  ,yr_qtr3:'sum'                                                                           \n",
    "                                                                                                  ,yr_qtr2:'sum'\n",
    "                                                                                                  ,yr_qtr1:'sum'})\n",
    "\n",
    "# unpivot the dataset\n",
    "leave_approved_data = leave_approved_data.melt(id_vars=['Masked Code','Leave Type'], var_name='YearQuarter', value_name='RequestUtilLeaves')\n",
    "\n",
    "leave_approved_data[['LeaveCode','LeaveRequest']] = leave_approved_data['Leave Type'].str.split(\"-\", expand=True) \n",
    "\n",
    "# create a key\n",
    "leave_approved_data['Key'] = leave_approved_data['Masked Code'] + leave_approved_data['LeaveCode'] + leave_approved_data['YearQuarter']\n",
    "\n",
    "\n",
    "# filter cancelled leaves requests for leave utilization calculation\n",
    "leave_cancelled_data = leave_data[(leave_data['Leave Type']== 'LE-Cancellation') | \n",
    "                            (leave_data['Leave Type']== 'SPL-Cancellation')|\n",
    "                            (leave_data['Leave Type']== 'HTO-Cancellation')|\n",
    "                            (leave_data['Leave Type']== 'LWP-Cancellation')|\n",
    "                            (leave_data['Leave Type']== 'ML-Cancellation') |\n",
    "                            (leave_data['Leave Type']== 'PL-Cancellation') |\n",
    "                            (leave_data['Leave Type']== 'BL-Cancellation') |\n",
    "                            (leave_data['Leave Type']== 'RTL-Cancellation')]\n",
    "\n",
    "#  separate approved and rejected leaves data \n",
    "leave_cancelled_data = leave_cancelled_data[leave_cancelled_data['Status']== 'Approved']\n",
    "\n",
    "# calculate the date differences (Saturday and Sunday included)\n",
    "leave_cancelled_data['LCQEQSDiff_QB4'] = (q4_end - q4_start) + timedelta(days=1)\n",
    "leave_cancelled_data['LCEQSDiff_QB4'] = (leave_cancelled_data['To Date']  - q4_start) + timedelta(days=1)\n",
    "leave_cancelled_data['QELCSDiff_QB4'] = (q4_end - leave_cancelled_data['From Date']) + timedelta(days=1)\n",
    "leave_cancelled_data['LCELCSDiff_QB4'] = (leave_cancelled_data['To Date']  - leave_cancelled_data['From Date']) + timedelta(days=1)\n",
    "\n",
    "leave_cancelled_data['LCQEQSDiff_QB3'] = (q3_end - q3_start) + timedelta(days=1)\n",
    "leave_cancelled_data['LCEQSDiff_QB3'] = (leave_cancelled_data['To Date']  - q3_start) + timedelta(days=1)\n",
    "leave_cancelled_data['QELCSDiff_QB3'] = (q3_end - leave_cancelled_data['From Date']) + timedelta(days=1)\n",
    "leave_cancelled_data['LCELCSDiff_QB3'] = (leave_cancelled_data['To Date']  - leave_cancelled_data['From Date']) + timedelta(days=1)\n",
    "\n",
    "leave_cancelled_data['LCQEQSDiff_QB2'] = (q2_end - q2_start) + timedelta(days=1)\n",
    "leave_cancelled_data['LCEQSDiff_QB2'] = (leave_cancelled_data['To Date']  - q2_start) + timedelta(days=1)\n",
    "leave_cancelled_data['QELCSDiff_QB2'] = (q2_end - leave_cancelled_data['From Date']) + timedelta(days=1)\n",
    "leave_cancelled_data['LCELCSDiff_QB2'] = (leave_cancelled_data['To Date']  - leave_cancelled_data['From Date']) + timedelta(days=1)\n",
    "\n",
    "leave_cancelled_data['LCQEQSDiff_QB1'] = (q1_end - q1_start) + timedelta(days=1)\n",
    "leave_cancelled_data['LCEQSDiff_QB1'] = (leave_cancelled_data['To Date']  - q1_start) + timedelta(days=1)\n",
    "leave_cancelled_data['QELCSDiff_QB1'] = (q1_end - leave_cancelled_data['From Date']) + timedelta(days=1)\n",
    "leave_cancelled_data['LCELCSDiff_QB1'] = (leave_cancelled_data['To Date']  - leave_cancelled_data['From Date']) + timedelta(days=1)\n",
    "\n",
    "# calculate the number of days an employee has utilized leaves in each quarter\n",
    "# leave on 4th quarter\n",
    "cols_leavecancel_QB4 = ['From Date', 'To Date', 'LCQEQSDiff_QB4',  'LCEQSDiff_QB4', 'QELCSDiff_QB4', 'LCELCSDiff_QB4']\n",
    "leave_cancelled_data[yr_qtr4] = quarter_days_allocator(leave_cancelled_data, cols_leavecancel_QB4, q4_start, q4_end)\n",
    "\n",
    "# leave on 3rd quarter\n",
    "cols_leavecancel_QB3 = ['From Date', 'To Date', 'LCQEQSDiff_QB3',  'LCEQSDiff_QB3', 'QELCSDiff_QB3', 'LCELCSDiff_QB3']\n",
    "leave_cancelled_data[yr_qtr3] = quarter_days_allocator(leave_cancelled_data, cols_leavecancel_QB3, q3_start, q3_end)\n",
    "\n",
    "# leave on 2nd quarter\n",
    "cols_leavecancel_QB2 = ['From Date', 'To Date', 'LCQEQSDiff_QB2',  'LCEQSDiff_QB2', 'QELCSDiff_QB2', 'LCELCSDiff_QB2']\n",
    "leave_cancelled_data[yr_qtr2] = quarter_days_allocator(leave_cancelled_data, cols_leavecancel_QB2, q2_start, q2_end)\n",
    "\n",
    "# leave on 1st quarter\n",
    "cols_leavecancel_QB1 = ['From Date', 'To Date', 'LCQEQSDiff_QB1',  'LCEQSDiff_QB1', 'QELCSDiff_QB1', 'LCELCSDiff_QB1']\n",
    "leave_cancelled_data[yr_qtr1] = quarter_days_allocator(leave_cancelled_data, cols_leavecancel_QB1, q1_start, q1_end)\n",
    "\n",
    "\n",
    "\n",
    "# selecting required columns\n",
    "leave_cancelled_data = leave_cancelled_data[['Masked Code', 'Leave Type', 'TotalLeaveDays',yr_qtr4,yr_qtr3,yr_qtr2,yr_qtr1]]\n",
    "\n",
    "# converting from days to int\n",
    "leave_cancelled_data[yr_qtr4] = (leave_cancelled_data[yr_qtr4] / np.timedelta64(1, 'D')).astype(int)\n",
    "leave_cancelled_data[yr_qtr3] = (leave_cancelled_data[yr_qtr3] / np.timedelta64(1, 'D')).astype(int)\n",
    "leave_cancelled_data[yr_qtr2] = (leave_cancelled_data[yr_qtr2] / np.timedelta64(1, 'D')).astype(int)\n",
    "leave_cancelled_data[yr_qtr1] = (leave_cancelled_data[yr_qtr1] / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "# grouping by employees\n",
    "leave_cancelled_data = leave_cancelled_data.groupby(['Masked Code','Leave Type'], as_index=False).agg({yr_qtr4:'sum'\n",
    "                                                                                                  ,yr_qtr3:'sum'                                                                                 \n",
    "                                                                                                  ,yr_qtr2:'sum'\n",
    "                                                                                                  ,yr_qtr1:'sum'})\n",
    "\n",
    "# unpivot the dataset\n",
    "leave_cancelled_data = leave_cancelled_data.melt(id_vars=['Masked Code','Leave Type'], var_name='YearQuarter', value_name='CancelUtilLeaves')\n",
    "\n",
    "leave_cancelled_data[['LeaveCode','LeaveCancel']] = leave_cancelled_data['Leave Type'].str.split(\"-\", expand=True) \n",
    "\n",
    "# create a key\n",
    "leave_cancelled_data['Key'] = leave_cancelled_data['Masked Code'] + leave_cancelled_data['LeaveCode'] + leave_cancelled_data['YearQuarter']\n",
    "\n",
    "leave_cancelled_data = leave_cancelled_data[['Key', 'CancelUtilLeaves']]\n",
    "\n",
    "leave_approved_data = pd.merge(leave_approved_data, leave_cancelled_data, on='Key', how='left')\n",
    "\n",
    "leave_approved_data['CancelUtilLeaves'] = leave_approved_data['CancelUtilLeaves'].fillna(0)\n",
    "\n",
    "del leave_approved_data['Key']\n",
    "\n",
    "leave_approved_data['UtilLeaves'] = leave_approved_data['RequestUtilLeaves'] - leave_approved_data['CancelUtilLeaves']\n",
    "\n",
    "leave_approved_data['UtilLeaves'] = np.where(leave_approved_data['UtilLeaves'] < 0, 0, leave_approved_data['UtilLeaves'])\n",
    "\n",
    "\n",
    "# enrich with leave approved data\n",
    "# by latest year\n",
    "leave_approved_data_latestYr = leave_approved_data.copy(deep=True)\n",
    "\n",
    "# filter the latest year\n",
    "leave_approved_data_latestYr = leave_approved_data_latestYr[(leave_approved_data_latestYr['YearQuarter'] == yr_qtr1) | \n",
    "                                                                  (leave_approved_data_latestYr['YearQuarter'] == yr_qtr2) |\n",
    "                                                                  (leave_approved_data_latestYr['YearQuarter'] == yr_qtr3) |\n",
    "                                                                  (leave_approved_data_latestYr['YearQuarter'] == yr_qtr4)]\n",
    "# subset required columns\n",
    "leave_approved_data_latestYr = leave_approved_data_latestYr[['Masked Code', 'Leave Type', 'UtilLeaves']]\n",
    "leave_approved_data_latestYr = leave_approved_data_latestYr.groupby(['Masked Code', 'Leave Type'], as_index=False).agg({ 'UtilLeaves':'sum'})\n",
    "\n",
    "# create flags\n",
    "leave_approved_data_latestYr['MLPL_flag'] = np.where(((leave_approved_data_latestYr['Leave Type'] == 'ML-Request')|(leave_approved_data_latestYr['Leave Type'] == 'PL-Request')), 1, 0)\n",
    "leave_approved_data_latestYr['BL_flag'] = np.where((leave_approved_data_latestYr['Leave Type'] == 'BL-Request'), 1, 0)\n",
    "leave_approved_data_latestYr['RTL_flag'] = np.where((leave_approved_data_latestYr['Leave Type'] == 'RTL-Request'), 1, 0)\n",
    "leave_approved_data_latestYr['LWP_leaves'] = np.where((leave_approved_data_latestYr['Leave Type'] == 'LWP-Request'), leave_approved_data_latestYr['UtilLeaves'], 0)\n",
    "leave_approved_data_latestYr['LatestYr_Leave_Util'] = np.where(\n",
    "                                                        ((leave_approved_data_latestYr['Leave Type'] == 'LWP-Request')|\n",
    "                                                         (leave_approved_data_latestYr['Leave Type'] == 'HTO-Request')|\n",
    "                                                         (leave_approved_data_latestYr['Leave Type'] == 'LE-Request') |\n",
    "                                                         (leave_approved_data_latestYr['Leave Type'] == 'SPL-Request'))\n",
    "                                                         ,leave_approved_data_latestYr['UtilLeaves'], 0)\n",
    "\n",
    "leave_approved_data_latestYr = leave_approved_data_latestYr.groupby(['Masked Code'], as_index=False).agg({ 'MLPL_flag':'sum',\n",
    "                                                                                                               'BL_flag':'sum',\n",
    "                                                                                                               'RTL_flag':'sum',\n",
    "                                                                                                               'LWP_leaves':'sum',\n",
    "                                                                                                               'LatestYr_Leave_Util':'sum'})\n",
    "# merge leave utilization data for latest year\n",
    "DATA = pd.merge(DATA, leave_approved_data_latestYr, on='Masked Code', how='left')\n",
    "\n",
    "# by latest quarter\n",
    "# filter the required leave types\n",
    "leave_approved_data_latestQtr = leave_approved_data[(leave_approved_data['Leave Type'] == 'LE-Request') |\n",
    "                                                    (leave_approved_data['Leave Type'] == 'HTO-Request')|\n",
    "                                                    (leave_approved_data['Leave Type'] == 'SPL-Request')|\n",
    "                                                    (leave_approved_data['Leave Type'] == 'LWP-Request')] \n",
    "\n",
    "# filter latest quarter\n",
    "leave_approved_data_latestQtr = leave_approved_data_latestQtr[(leave_approved_data_latestQtr['YearQuarter'] == yr_qtr1)]\n",
    "\n",
    "#LWP by quarter\n",
    "leave_approved_data_latestQtr_LWP = leave_approved_data_latestQtr.copy(deep = True)\n",
    "leave_approved_data_latestQtr_LWP = leave_approved_data_latestQtr_LWP[(leave_approved_data_latestQtr_LWP['Leave Type'] == 'LWP-Request')]                                                         \n",
    "leave_approved_data_latestQtr_LWP = leave_approved_data_latestQtr_LWP[['Masked Code', 'UtilLeaves']]\n",
    "leave_approved_data_latestQtr_LWP.columns = ['Masked Code', 'LatestQtr_LWP_Leave_Util']\n",
    "DATA = pd.merge(DATA, leave_approved_data_latestQtr_LWP, on='Masked Code', how='left')                                                                              \n",
    "\n",
    "leave_approved_data_latestQtr = leave_approved_data_latestQtr[['Masked Code', 'UtilLeaves']]\n",
    "leave_approved_data_latestQtr.columns = ['Masked Code', 'LatestQtr_Leave_Util']\n",
    "leave_approved_data_latestQtr = leave_approved_data_latestQtr.groupby(['Masked Code'], as_index=False).agg({ 'LatestQtr_Leave_Util':'sum'})\n",
    "# merge leave utilization data for latest quarter\n",
    "DATA = pd.merge(DATA, leave_approved_data_latestQtr, on='Masked Code', how='left')\n",
    "\n",
    "# merge leave request declined data\n",
    "leave_declined_data= leave_declined_data[(leave_declined_data['YearQuarter'] == yr_qtr1)]\n",
    "leave_declined_data= leave_declined_data[['Masked Code', 'CountOfRejections']]\n",
    "DATA = pd.merge(DATA, leave_declined_data, on='Masked Code', how='left')\n",
    "\n",
    "# fill missing values with 0\n",
    "DATA['CountOfRejections'] = DATA['CountOfRejections'].fillna(0)\n",
    "\n",
    "print(\"Successfully executed Leave Ledger Data\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\Compensation Data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed Compensation Data\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "compensation_data = pd.read_excel('Compensation Data.xlsx' , sheet_name ='Sheet1')\n",
    "\n",
    "\n",
    "\n",
    "# subset required columns\n",
    "compensation_data = compensation_data[['Encrypted Emp IDs', 'DoJ','Actual Releiving Date',\n",
    "                                   'Masked FP Latest Year June','Masked FP Latest Year Quarter',\n",
    "                                   'Masked PP Latest Year']]\n",
    "\n",
    "compensation_data.columns = ['Masked Code','DoJ','RelievingDate','FP_LatestYr_Jun',\n",
    "                         'FP_LatestYrQtr','PP_LatestYr']\n",
    "\n",
    "# converting date to required format\n",
    "compensation_data['DoJ'] = pd.to_datetime(compensation_data['DoJ'])\n",
    "compensation_data['RelievingDate'] = pd.to_datetime(compensation_data['RelievingDate'], errors='coerce').dt.date                                             \n",
    "\n",
    "# calculating yearly increments\n",
    "compensation_data['Increment_LatestYr'] = np.where(\n",
    "                                           (compensation_data['FP_LatestYr_Jun'] != 0),\n",
    "                                                (compensation_data['FP_LatestYrQtr']/compensation_data['FP_LatestYr_Jun']) - 1,\n",
    "                                                    0)\n",
    "\n",
    "#logic added for latest run\n",
    "compensation_data['FP_LatestYr_Jul'] = compensation_data['FP_LatestYrQtr']\n",
    "\n",
    "# deleting 2017 data\n",
    "del compensation_data['DoJ']\n",
    "\n",
    "# enrich with Compensation Data\n",
    "DATA = pd.merge(DATA, compensation_data, on='Masked Code', how='left')\n",
    "\n",
    "# calculating latest year percent salary hike\n",
    "DATA['LatestYrSalaryHike'] =  DATA['Increment_LatestYr'] * 100\n",
    "\n",
    "print(\"Successfully executed Compensation Data\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code\\\\Input Data\\\\Oct 2020\\\\Travel Data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully executed Travel Data\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "travel_data_domestic = pd.read_excel('TC Air Domestic Data.xlsx', sheet_name = \"Sheet1\")\n",
    "travel_data_domestic['ItineraryType'] = 'Domestic'\n",
    "travel_data_domestic['RefInvoiceNumber'] = np.NaN\n",
    "travel_data_domestic['SystemInvoiceId'] = np.NaN\n",
    "travel_data_domestic = travel_data_domestic[['TRIP CODE', 'SystemInvoiceId','RefInvoiceNumber',\"Masked Traveller's Emp Id\",'ItineraryType','ONEWAY/RETURN','Date of Departure', 'Date of Arrival']]\n",
    "travel_data_domestic.columns = ['TripReference', 'SystemInvoiceId', 'RefInvoiceNumber', 'Masked Code', 'ItineraryType', 'JourneyType', 'TripStartDate', 'TripEndDate']\n",
    "\n",
    "travel_data_international = pd.read_excel('TC Air International Data.xlsx', sheet_name = \"Sheet1\")\n",
    "travel_data_international['ItineraryType'] = 'International'\n",
    "travel_data_international['RefInvoiceNumber'] = np.NaN\n",
    "travel_data_international['SystemInvoiceId'] = np.NaN\n",
    "travel_data_international = travel_data_international[['TRIP CODE', 'SystemInvoiceId','RefInvoiceNumber',\"Masked Traveller's Emp id\",'ItineraryType','Journey Type','Departure Date', 'Arrival Date']]\n",
    "travel_data_international.columns = ['TripReference', 'SystemInvoiceId', 'RefInvoiceNumber', 'Masked Code', 'ItineraryType', 'JourneyType', 'TripStartDate', 'TripEndDate']\n",
    "\n",
    "\n",
    "\n",
    "# concate all the 3 datasets\n",
    "travel_data = pd.concat([travel_data_domestic,travel_data_international], axis=0)\n",
    "\n",
    "# remove trip ref which is 0 or null\n",
    "travel_data = travel_data[(travel_data['TripReference'].notnull()) | (travel_data['TripReference'] != 0)]\n",
    "\n",
    "# converting date to required format\n",
    "travel_data['TripStartDate'] = pd.to_datetime(travel_data['TripStartDate']).dt.date\n",
    "travel_data['TripEndDate'] = pd.to_datetime(travel_data['TripEndDate']).dt.date\n",
    "\n",
    "travel_data['TripStartDate'] = pd.to_datetime(travel_data['TripStartDate'])\n",
    "travel_data['TripEndDate'] = pd.to_datetime(travel_data['TripEndDate'])\n",
    "\n",
    "# sort the data\n",
    "travel_data = travel_data.sort_values(by=['Masked Code', 'TripStartDate', 'TripEndDate'], ascending=True)\n",
    "\n",
    "# create reissued dataset and actual ones\n",
    "travel_data_reissued = travel_data[['RefInvoiceNumber']]\n",
    "travel_data_reissued = travel_data_reissued.dropna()\n",
    "travel_data_reissued['Reissued'] = 1\n",
    "\n",
    "# removing tickets which has been reissued\n",
    "travel_data = pd.merge(travel_data, travel_data_reissued, left_on='SystemInvoiceId', right_on='RefInvoiceNumber', how='left')\n",
    "\n",
    "travel_data = travel_data[travel_data['Reissued'] != 1]\n",
    "\n",
    "# subset required columns\n",
    "travel_data = travel_data[['TripReference', 'Masked Code', 'ItineraryType', 'JourneyType', 'TripStartDate', 'TripEndDate']]\n",
    "\n",
    "# extract year and quarter from the Trip Start Date\n",
    "travel_data['Quarter'] = pd.PeriodIndex(travel_data['TripStartDate'], freq='Q').strftime('Q%q')\n",
    "travel_data['Year'] = pd.PeriodIndex(travel_data['TripStartDate'], freq='Y').strftime('%Y')\n",
    "travel_data['YearQuarter'] = travel_data['Year'] + travel_data['Quarter']\n",
    "\n",
    "# calculate travel metrics related to travel frequency\n",
    "travel_data_freq = travel_data[['TripReference', 'Masked Code', 'ItineraryType', 'YearQuarter']]\n",
    "\n",
    "travel_data_freq = travel_data_freq.groupby(['Masked Code', 'YearQuarter', 'ItineraryType']).TripReference.nunique()\n",
    "\n",
    "travel_data_freq = pd.DataFrame(travel_data_freq).reset_index()\n",
    "\n",
    "travel_data_freq['DomesticTravelFreq'] = np.where((travel_data_freq['ItineraryType'] == \"Domestic\"), travel_data_freq['TripReference'], 0)\n",
    "travel_data_freq['InternationalTravelFreq'] = np.where((travel_data_freq['ItineraryType'] == \"International\"), travel_data_freq['TripReference'], 0)\n",
    "\n",
    "travel_data_freq = travel_data_freq.groupby(['Masked Code', 'YearQuarter'], as_index=False).agg({'DomesticTravelFreq':'sum'\n",
    "                                                                              ,'InternationalTravelFreq':'sum'})\n",
    "\n",
    "# calculate total travel frequency\n",
    "travel_data_freq['TotalTravelFreq'] = travel_data_freq['DomesticTravelFreq'] + travel_data_freq['InternationalTravelFreq']\n",
    "\n",
    "# create travel flags depending on travel type \n",
    "travel_data_freq['IsDomesticTravel'] = np.where((travel_data_freq['DomesticTravelFreq'] >= 1), 1, 0)\n",
    "travel_data_freq['IsInternationalTravel'] = np.where((travel_data_freq['InternationalTravelFreq'] >= 1), 1, 0)\n",
    "\n",
    "del travel_data_freq['DomesticTravelFreq']\n",
    "del travel_data_freq['InternationalTravelFreq']\n",
    "\n",
    "# calculate trip duration\n",
    "travel_data_duration = travel_data[['TripReference', 'Masked Code', 'JourneyType', 'TripStartDate', 'TripEndDate']]\n",
    "\n",
    "travel_data_duration['TripEndDate'] =  travel_data_duration.groupby(['TripReference', 'Masked Code'])['TripEndDate'].transform('max')\n",
    "\n",
    "travel_data_duration = travel_data_duration.drop_duplicates(subset = ['TripReference', 'Masked Code'], keep = 'first')\n",
    "\n",
    "travel_data_duration = travel_data_duration.reset_index(drop = True)\n",
    "\n",
    "travel_data_duration = travel_data_duration[['Masked Code', 'TripStartDate', 'TripEndDate']]\n",
    "\n",
    "\n",
    "# calculate the date differences (Saturday and Sunday included)\n",
    "travel_data_duration['TQEQSDiff_QB4'] = (q4_end - q4_start) + timedelta(days=1)\n",
    "travel_data_duration['TEQSDiff_QB4'] = (travel_data_duration['TripEndDate']  - q4_start) + timedelta(days=1)\n",
    "travel_data_duration['QETSDiff_QB4'] = (q4_end - travel_data_duration['TripStartDate']) + timedelta(days=1)\n",
    "travel_data_duration['TETSDiff_QB4'] = (travel_data_duration['TripEndDate']  - travel_data_duration['TripStartDate']) + timedelta(days=1)\n",
    "\n",
    "travel_data_duration['TQEQSDiff_QB3'] = (q3_end - q3_start) + timedelta(days=1)\n",
    "travel_data_duration['TEQSDiff_QB3'] = (travel_data_duration['TripEndDate']  - q3_start) + timedelta(days=1)\n",
    "travel_data_duration['QETSDiff_QB3'] = (q3_end - travel_data_duration['TripStartDate']) + timedelta(days=1)\n",
    "travel_data_duration['TETSDiff_QB3'] = (travel_data_duration['TripEndDate']  - travel_data_duration['TripStartDate']) + timedelta(days=1)\n",
    "\n",
    "travel_data_duration['TQEQSDiff_QB2'] = (q2_end - q2_start) + timedelta(days=1)\n",
    "travel_data_duration['TEQSDiff_QB2'] = (travel_data_duration['TripEndDate']  - q2_start) + timedelta(days=1)\n",
    "travel_data_duration['QETSDiff_QB2'] = (q2_end - travel_data_duration['TripStartDate']) + timedelta(days=1)\n",
    "travel_data_duration['TETSDiff_QB2'] = (travel_data_duration['TripEndDate']  - travel_data_duration['TripStartDate']) + timedelta(days=1)\n",
    "\n",
    "travel_data_duration['TQEQSDiff_QB1'] = (q1_end - q1_start) + timedelta(days=1)\n",
    "travel_data_duration['TEQSDiff_QB1'] = (travel_data_duration['TripEndDate']  - q1_start) + timedelta(days=1)\n",
    "travel_data_duration['QETSDiff_QB1'] = (q1_end - travel_data_duration['TripStartDate']) + timedelta(days=1)\n",
    "travel_data_duration['TETSDiff_QB1'] = (travel_data_duration['TripEndDate']  - travel_data_duration['TripStartDate']) + timedelta(days=1)\n",
    "\n",
    "# calculate the number of days an employee has travelled in each quarter\n",
    "# travel on 4th quarter\n",
    "cols_travel_QB4 = ['TripStartDate', 'TripEndDate', 'TQEQSDiff_QB4',  'TEQSDiff_QB4', 'QETSDiff_QB4', 'TETSDiff_QB4']\n",
    "travel_data_duration[yr_qtr4] = quarter_days_allocator(travel_data_duration, cols_travel_QB4, q4_start, q4_end)\n",
    "\n",
    "# travel on 3rd quarter\n",
    "cols_travel_QB3 = ['TripStartDate', 'TripEndDate', 'TQEQSDiff_QB3',  'TEQSDiff_QB3', 'QETSDiff_QB3', 'TETSDiff_QB3']\n",
    "travel_data_duration[yr_qtr3] = quarter_days_allocator(travel_data_duration, cols_travel_QB3, q3_start, q3_end)\n",
    "\n",
    "# travel on 2nd quarter\n",
    "cols_travel_QB2 = ['TripStartDate', 'TripEndDate', 'TQEQSDiff_QB2',  'TEQSDiff_QB2', 'QETSDiff_QB2', 'TETSDiff_QB2']\n",
    "travel_data_duration[yr_qtr2] = quarter_days_allocator(travel_data_duration, cols_travel_QB2, q2_start, q2_end)\n",
    "\n",
    "# travel on 1st quarter\n",
    "cols_travel_QB1 = ['TripStartDate', 'TripEndDate', 'TQEQSDiff_QB1',  'TEQSDiff_QB1', 'QETSDiff_QB1', 'TETSDiff_QB1']\n",
    "travel_data_duration[yr_qtr1] = quarter_days_allocator(travel_data_duration, cols_travel_QB1, q1_start, q1_end)\n",
    "\n",
    "\n",
    "\n",
    "# selecting required columns\n",
    "travel_data_duration = travel_data_duration[['Masked Code',yr_qtr4,yr_qtr3,yr_qtr2,yr_qtr1]]\n",
    "\n",
    "# converting from days to int\n",
    "travel_data_duration[yr_qtr4] = (travel_data_duration[yr_qtr4] / np.timedelta64(1, 'D')).astype(int)\n",
    "travel_data_duration[yr_qtr3] = (travel_data_duration[yr_qtr3] / np.timedelta64(1, 'D')).astype(int)\n",
    "travel_data_duration[yr_qtr2] = (travel_data_duration[yr_qtr2] / np.timedelta64(1, 'D')).astype(int)\n",
    "travel_data_duration[yr_qtr1] = (travel_data_duration[yr_qtr1] / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "# grouping by employees\n",
    "travel_data_duration = travel_data_duration.groupby(['Masked Code'], as_index=False).agg({yr_qtr4:'sum'\n",
    "                                                                                      ,yr_qtr3:'sum'                                                                               \n",
    "                                                                                      ,yr_qtr2:'sum'\n",
    "                                                                                      ,yr_qtr1:'sum'})\n",
    "\n",
    "# unpivot the dataset\n",
    "travel_data_duration = travel_data_duration.melt(id_vars=['Masked Code'], var_name='YearQuarter', value_name='TravelDuration')\n",
    "\n",
    "\n",
    "# enrich with travel data\n",
    "# travel frequency data\n",
    "# by latest year\n",
    "travel_data_freq_latestYr = travel_data_freq[(travel_data_freq['YearQuarter'] == yr_qtr1) | \n",
    "                                              (travel_data_freq['YearQuarter'] == yr_qtr2) |\n",
    "                                              (travel_data_freq['YearQuarter'] == yr_qtr3) |\n",
    "                                              (travel_data_freq['YearQuarter'] == yr_qtr4)]\n",
    "# subset required columns\n",
    "travel_data_freq_latestYr = travel_data_freq_latestYr[['Masked Code', 'TotalTravelFreq']]\n",
    "travel_data_freq_latestYr = travel_data_freq_latestYr.groupby(['Masked Code'], as_index=False).agg({ 'TotalTravelFreq':'sum'})\n",
    "travel_data_freq_latestYr.columns = ['Masked Code', 'LatestYr_TotalTravelFreq']\n",
    "\n",
    "# merge with Quarter Block\n",
    "DATA = pd.merge(DATA, travel_data_freq_latestYr, on='Masked Code', how='left')\n",
    "\n",
    "# fill missing values with 0\n",
    "DATA['LatestYr_TotalTravelFreq'] = DATA['LatestYr_TotalTravelFreq'].fillna(0)\n",
    "\n",
    "# by latest quarter\n",
    "travel_data_freq_latestQtr = travel_data_freq[(travel_data_freq['YearQuarter'] == yr_qtr1)]\n",
    "\n",
    "# subset required columns\n",
    "travel_data_freq_latestQtr = travel_data_freq_latestQtr[['Masked Code', 'TotalTravelFreq', 'IsDomesticTravel', 'IsInternationalTravel']]\n",
    "travel_data_freq_latestQtr.columns = ['Masked Code', 'LatestQtr_TotalTravelFreq', 'LatestQtr_HasDomesticTravel', 'LatestQtr_HasInternationalTravel']\n",
    "\n",
    "# merge with Quarter Block\n",
    "DATA = pd.merge(DATA, travel_data_freq_latestQtr, on='Masked Code', how='left')\n",
    "\n",
    "# fill missing values with 0\n",
    "DATA['LatestQtr_TotalTravelFreq'] = DATA['LatestQtr_TotalTravelFreq'].fillna(0)\n",
    "DATA['LatestQtr_HasDomesticTravel'] = DATA['LatestQtr_HasDomesticTravel'].fillna(0)\n",
    "DATA['LatestQtr_HasInternationalTravel'] = DATA['LatestQtr_HasInternationalTravel'].fillna(0)\n",
    "\n",
    "# travel duration data\n",
    "# by latest year\n",
    "travel_data_duration_latestYr = travel_data_duration[(travel_data_duration['YearQuarter'] == yr_qtr1) | \n",
    "                                                      (travel_data_duration['YearQuarter'] == yr_qtr2) |\n",
    "                                                      (travel_data_duration['YearQuarter'] == yr_qtr3) |\n",
    "                                                      (travel_data_duration['YearQuarter'] == yr_qtr4)]\n",
    "# subset required columns\n",
    "travel_data_duration_latestYr = travel_data_duration_latestYr[['Masked Code', 'TravelDuration']]\n",
    "travel_data_duration_latestYr = travel_data_duration_latestYr.groupby(['Masked Code'], as_index=False).agg({'TravelDuration':'sum'})\n",
    "travel_data_duration_latestYr.columns = ['Masked Code', 'LatestYr_TravelDuration']\n",
    "\n",
    "# merge with Quarter Block\n",
    "DATA = pd.merge(DATA, travel_data_duration_latestYr, on='Masked Code', how='left')\n",
    "\n",
    "# fill missing values with 0\n",
    "DATA['LatestYr_TravelDuration'] = DATA['LatestYr_TravelDuration'].fillna(0)\n",
    "\n",
    "# by latest quarter\n",
    "travel_data_duration_latestQtr = travel_data_duration[(travel_data_duration['YearQuarter'] == yr_qtr1)]\n",
    "\n",
    "# subset required columns\n",
    "travel_data_duration_latestQtr = travel_data_duration_latestQtr[['Masked Code', 'TravelDuration']]\n",
    "travel_data_duration_latestQtr.columns = ['Masked Code', 'LatestQtr_TravelDuration']\n",
    "\n",
    "# merge with Quarter Block\n",
    "DATA = pd.merge(DATA, travel_data_duration_latestQtr, on='Masked Code', how='left')\n",
    "\n",
    "# fill missing values with 0\n",
    "DATA['LatestQtr_TravelDuration'] = DATA['LatestQtr_TravelDuration'].fillna(0)\n",
    "\n",
    "print(\"Successfully executed Travel Data\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Masked Code',\n",
       " 'DATE_OF_BIRTH',\n",
       " 'Masked Gender',\n",
       " 'MARITAL_STATUS',\n",
       " 'WEDDING_DATE',\n",
       " 'Masked Location',\n",
       " 'PERMNANT_STATE',\n",
       " 'Masked Entity',\n",
       " 'Masked Status',\n",
       " 'Masked Department',\n",
       " 'Masked Function',\n",
       " 'Masked Grade',\n",
       " 'NOTICE_PERIOD_IN_DAYS',\n",
       " 'DATE_OF_JOINING',\n",
       " 'DATE_OF_LEAVING',\n",
       " 'Masked RM ID',\n",
       " 'PARTNER_NAME',\n",
       " 'REPORTING_TO',\n",
       " 'EMPLOYEE_STATUS',\n",
       " 'AGE',\n",
       " 'YEARS_SINCE_MARRIAGE',\n",
       " 'YEARS_AT_COMPANY',\n",
       " 'Office City',\n",
       " 'Office State',\n",
       " 'IsSameState',\n",
       " 'Resignation Date',\n",
       " 'Attrition',\n",
       " 'Encrypted Emp ID_x',\n",
       " 'EmpPerformanceRating',\n",
       " 'Encrypted Emp ID_y',\n",
       " 'RMPerformanceRating',\n",
       " 'EffectiveDate',\n",
       " 'Grade',\n",
       " 'IsPromoted',\n",
       " 'IsPromoted_LatestQtr',\n",
       " 'IsPromoted_PreviousQtr',\n",
       " 'LatestYr_BILL_UTIL',\n",
       " 'LatestYr_PROD_NONBILL_UTIL',\n",
       " 'LatestYr_NONPROD_NONBILL_UTIL',\n",
       " 'LatestQtr_BILL_UTIL',\n",
       " 'LatestQtr_PROD_NONBILL_UTIL',\n",
       " 'LatestQtr_NONPROD_NONBILL_UTIL',\n",
       " 'IS_EXPERIENCED',\n",
       " 'NUM_PRIOR_COMPANIES',\n",
       " 'PRIOR_EXP',\n",
       " 'PRIOR_AVG_TENURE',\n",
       " 'From',\n",
       " 'From.1',\n",
       " 'From.2',\n",
       " 'From.3',\n",
       " 'From.4',\n",
       " 'From.5',\n",
       " 'From.6',\n",
       " 'From.7',\n",
       " 'From.8',\n",
       " 'From.9',\n",
       " 'From.10',\n",
       " 'From.11',\n",
       " 'From.12',\n",
       " 'From.13',\n",
       " 'TOTAL_WORK_EXP',\n",
       " 'LAST5YRS_CUTOFF_DATE',\n",
       " 'LAST5YRS_COMP_WORKED',\n",
       " 'LatestQtrTechLearnHours',\n",
       " 'LatestQtrProfLeadDevLearnHours',\n",
       " 'LatestYrTechLearnHours',\n",
       " 'LatestYrProfLeadDevLearnHours',\n",
       " 'LatestQtrTechNumCourses',\n",
       " 'LatestQtrProfLeadDevNumCourses',\n",
       " 'LatestYrTechNumCourses',\n",
       " 'LatestYrProfLeadDevNumCourses',\n",
       " 'NumOfRewards',\n",
       " 'NumOfRecognitions',\n",
       " 'TotalRnR',\n",
       " 'IsRewarded',\n",
       " 'IsRecognized',\n",
       " 'Amount(Rs)',\n",
       " 'LatestQtr_snapshot_composite_score',\n",
       " 'LatestYr_snapshot_composite_score',\n",
       " 'LatestQtrProjectDays',\n",
       " 'LatestYrProjectDays',\n",
       " 'LatestQtrPerfSnapshot',\n",
       " 'LatestQtrOtherSnapshot',\n",
       " 'LatestQtrTotalSnapshot',\n",
       " 'LatestYrPerfSnapshot',\n",
       " 'LatestYrOtherSnapshot',\n",
       " 'LatestYrTotalSnapshot',\n",
       " 'MLPL_flag',\n",
       " 'BL_flag',\n",
       " 'RTL_flag',\n",
       " 'LWP_leaves',\n",
       " 'LatestYr_Leave_Util',\n",
       " 'LatestQtr_LWP_Leave_Util',\n",
       " 'LatestQtr_Leave_Util',\n",
       " 'CountOfRejections',\n",
       " 'RelievingDate',\n",
       " 'FP_LatestYr_Jun',\n",
       " 'FP_LatestYrQtr',\n",
       " 'PP_LatestYr',\n",
       " 'Increment_LatestYr',\n",
       " 'FP_LatestYr_Jul',\n",
       " 'LatestYrSalaryHike',\n",
       " 'LatestYr_TotalTravelFreq',\n",
       " 'LatestQtr_TotalTravelFreq',\n",
       " 'LatestQtr_HasDomesticTravel',\n",
       " 'LatestQtr_HasInternationalTravel',\n",
       " 'LatestYr_TravelDuration',\n",
       " 'LatestQtr_TravelDuration']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(DATA.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yr_qtr1[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Attrition Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully handled Outliers\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create flag\n",
    "\n",
    "# DATA['YEAR'] = yr_qtr1[:4]\n",
    "\n",
    "# DATA['QUARTER'] = yr_qtr1[4:] \n",
    "\n",
    "DATA['YEAR'] = '2020'\n",
    "DATA['QUARTER'] = 'Q2'\n",
    "\n",
    "\n",
    "# assigning the data\n",
    "attrition_data= DATA\n",
    "\n",
    "# calculate YAL and latest grade for each quarter (didn't take into account active and inactive employees)\n",
    "attrition_data = attrition_data.sort_values(by=['Masked Code', 'YEAR', 'QUARTER'], ascending=True)\n",
    "\n",
    "# create temp date for YAL creation\n",
    "attrition_data['YAL_Date'] = np.where((attrition_data['EffectiveDate'].isnull()), attrition_data['DATE_OF_JOINING'], attrition_data['EffectiveDate'])\n",
    "\n",
    "# calculate YAL\n",
    "attrition_data['Days_at_Level'] = q1_end - attrition_data['YAL_Date']\n",
    "\n",
    "\n",
    "attrition_data['YAL'] = attrition_data['Days_at_Level']/np.timedelta64(1,'Y')\n",
    "\n",
    "# calculate latest grade for quarter\n",
    "attrition_data['LatestGrade'] = np.where((attrition_data['Grade'].isnull()), attrition_data['Masked Grade'], attrition_data['Grade'])\n",
    "\n",
    "# promoted flag\n",
    "attrition_data['IsPromoted'] = np.where((attrition_data['IsPromoted'] == 'Promoted'), 1 ,0)\n",
    "\n",
    "# capping project hours for snapshot data\n",
    "attrition_data['LatestQtrProjectDays'] = np.where((attrition_data['LatestQtrProjectDays'] > 91), 91, attrition_data['LatestQtrProjectDays'])\n",
    "attrition_data['LatestYrProjectDays'] = np.where((attrition_data['LatestYrProjectDays'] > 365), 365, attrition_data['LatestYrProjectDays'])\n",
    "\n",
    "# imputing outliers with median values for compensation data\n",
    "attrition_data.loc[attrition_data['LatestYrSalaryHike'] > 100 , 'LatestYrSalaryHike'] = attrition_data['LatestYrSalaryHike'].median()\n",
    "\n",
    "#  capping salary hike which are negative\n",
    "attrition_data['LatestYrSalaryHike'] = np.where((attrition_data['LatestYrSalaryHike'] < 0),\n",
    "                                                0, attrition_data['LatestYrSalaryHike'])\n",
    "\n",
    "# imputing nulls with 0 for LatestYrSalaryHike\n",
    "attrition_data['LatestYrSalaryHike'] = np.where((attrition_data['LatestYrSalaryHike'].isnull()),\n",
    "                                                0, attrition_data['LatestYrSalaryHike'])\n",
    "\n",
    "# replacing staff util negative values with 0\n",
    "attrition_data['LatestYr_BILL_UTIL'][attrition_data['LatestYr_BILL_UTIL'] < 0] = 0\n",
    "attrition_data['LatestYr_PROD_NONBILL_UTIL'][attrition_data['LatestYr_PROD_NONBILL_UTIL'] < 0] = 0\n",
    "attrition_data['LatestYr_NONPROD_NONBILL_UTIL'][attrition_data['LatestYr_NONPROD_NONBILL_UTIL'] < 0] = 0\n",
    "\n",
    "attrition_data['LatestQtr_BILL_UTIL'][attrition_data['LatestQtr_BILL_UTIL'] < 0] = 0\n",
    "attrition_data['LatestQtr_PROD_NONBILL_UTIL'][attrition_data['LatestQtr_PROD_NONBILL_UTIL'] < 0] = 0\n",
    "attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'][attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'] < 0] = 0\n",
    "\n",
    "# replace inf with 0\n",
    "attrition_data['LatestYr_BILL_UTIL'] = attrition_data['LatestYr_BILL_UTIL'].replace(np.inf, 0)\n",
    "attrition_data['LatestYr_PROD_NONBILL_UTIL'] = attrition_data['LatestYr_PROD_NONBILL_UTIL'].replace(np.inf, 0)\n",
    "attrition_data['LatestYr_NONPROD_NONBILL_UTIL'] = attrition_data['LatestYr_NONPROD_NONBILL_UTIL'].replace(np.inf, 0)\n",
    "attrition_data['LatestQtr_BILL_UTIL'] = attrition_data['LatestQtr_BILL_UTIL'].replace(np.inf, 0)\n",
    "attrition_data['LatestQtr_PROD_NONBILL_UTIL'] = attrition_data['LatestQtr_PROD_NONBILL_UTIL'].replace(np.inf, 0)\n",
    "attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'] = attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'].replace(np.inf, 0)\n",
    "\n",
    "#impute null values with 0\n",
    "attrition_data['LatestYr_BILL_UTIL'] = attrition_data['LatestYr_BILL_UTIL'].fillna(0)\n",
    "attrition_data['LatestYr_PROD_NONBILL_UTIL'] = attrition_data['LatestYr_PROD_NONBILL_UTIL'].fillna(0)\n",
    "attrition_data['LatestYr_NONPROD_NONBILL_UTIL'] = attrition_data['LatestYr_NONPROD_NONBILL_UTIL'].fillna(0)\n",
    "\n",
    "attrition_data['LatestQtr_BILL_UTIL'] = attrition_data['LatestQtr_BILL_UTIL'].fillna(0)\n",
    "attrition_data['LatestQtr_PROD_NONBILL_UTIL'] = attrition_data['LatestQtr_PROD_NONBILL_UTIL'].fillna(0)\n",
    "attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'] = attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'].fillna(0)\n",
    "\n",
    "# calculating percentile\n",
    "# staff Utilization by latest YEAR\n",
    "attrition_data['LatestYr_PER_BILL_UTIL'] = attrition_data.groupby(['YEAR', 'QUARTER', 'Masked Function', 'Masked Department', 'LatestGrade'])['LatestYr_BILL_UTIL'].rank(pct=True, method = 'min')\n",
    "attrition_data['LatestYr_PER_PROD_NONBILL_UTIL'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestYr_PROD_NONBILL_UTIL'].rank(pct=True, method = 'min')\n",
    "attrition_data['LatestYr_PER_NONPROD_NONBILL_UTIL'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestYr_NONPROD_NONBILL_UTIL'].rank(pct=True, method = 'min')\n",
    "\n",
    "# staff Utilization by latest QUARTER\n",
    "attrition_data['LatestQtr_PER_BILL_UTIL'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestQtr_BILL_UTIL'].rank(pct=True, method = 'min')\n",
    "attrition_data['LatestQtr_PER_PROD_NONBILL_UTIL'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestQtr_PROD_NONBILL_UTIL'].rank(pct=True, method = 'min')\n",
    "attrition_data['LatestQtr_PER_NONPROD_NONBILL_UTIL'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestQtr_NONPROD_NONBILL_UTIL'].rank(pct=True, method = 'min')\n",
    "\n",
    "#imputing null values of PP salary with 0\n",
    "attrition_data['PP_LatestYr']  = np.where((attrition_data['PP_LatestYr'].isnull()), 0, attrition_data['PP_LatestYr'])\n",
    "\n",
    "#salary variables\n",
    "attrition_data['FP_SALARY'] = attrition_data['FP_LatestYr_Jul']\n",
    "attrition_data['PP_SALARY'] =  attrition_data['PP_LatestYr']\n",
    "\n",
    "# percentile salary\n",
    "attrition_data['FP_PERCENTILE_SALARY'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['FP_LatestYr_Jul'].rank(pct=True, method = 'min')\n",
    "attrition_data['PP_PERCENTILE_SALARY'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['PP_LatestYr'].rank(pct=True, method = 'min')\n",
    "\n",
    "# calculate percentile for travel frequency\n",
    "attrition_data['PER_LatestYr_TotalTravelFreq'] = attrition_data.groupby(['YEAR','QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestYr_TotalTravelFreq'].rank(pct=True, method = 'min')\n",
    "\n",
    "# calculate percentile for travel frequency\n",
    "attrition_data['PER_LatestQtr_TotalTravelFreq'] = attrition_data.groupby(['YEAR','QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestQtr_TotalTravelFreq'].rank(pct=True, method = 'min')\n",
    "\n",
    "# calculate percentile for travel duration\n",
    "attrition_data['PER_LatestQtr_TravelDuration'] = attrition_data.groupby(['YEAR','QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestQtr_TravelDuration'].rank(pct=True, method = 'min')\n",
    "\n",
    "# calculate percentile for travel duration\n",
    "attrition_data['PER_LatestYr_TravelDuration'] = attrition_data.groupby(['YEAR','QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestYr_TravelDuration'].rank(pct=True, method = 'min')\n",
    "\n",
    "#YearQuarter\n",
    "attrition_data['YearQuarter'] = attrition_data['YEAR'] + attrition_data['QUARTER']\n",
    "\n",
    "# Leave utilised Year Quarter ratio\n",
    "attrition_data['Leave_Util_LatestQtrYr_Ratio'] = attrition_data['LatestQtr_Leave_Util']/attrition_data['LatestYr_Leave_Util']\n",
    "\n",
    "#  capping utilisation which are greater than 2\n",
    "attrition_data['LatestYr_BILL_UTIL'] = np.where((attrition_data['LatestYr_BILL_UTIL'] > 2), 2, attrition_data['LatestYr_BILL_UTIL'])\n",
    "attrition_data['LatestYr_PROD_NONBILL_UTIL'] = np.where((attrition_data['LatestYr_PROD_NONBILL_UTIL'] > 2), 2, attrition_data['LatestYr_PROD_NONBILL_UTIL'])\n",
    "attrition_data['LatestYr_NONPROD_NONBILL_UTIL'] = np.where((attrition_data['LatestYr_NONPROD_NONBILL_UTIL'] > 2), 2, attrition_data['LatestYr_NONPROD_NONBILL_UTIL'])\n",
    "\n",
    "attrition_data['LatestQtr_BILL_UTIL'] = np.where((attrition_data['LatestQtr_BILL_UTIL'] > 2), 2, attrition_data['LatestQtr_BILL_UTIL'])\n",
    "attrition_data['LatestQtr_PROD_NONBILL_UTIL'] = np.where((attrition_data['LatestQtr_PROD_NONBILL_UTIL'] > 2), 2, attrition_data['LatestQtr_PROD_NONBILL_UTIL'])\n",
    "attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'] = np.where((attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'] > 2), 2, attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'])\n",
    "\n",
    "print(\"Successfully handled Outliers\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Appending all the quarter blocks\n",
    "# # create flags for each quarter blocks\n",
    "# DATA['YEAR'] = '2018'\n",
    "# DATA['QUARTER'] = 'Q3'\n",
    "\n",
    "# DATA['YEAR'] = '2018'\n",
    "# DATA['QUARTER'] = 'Q4'\n",
    "\n",
    "# DATA['YEAR'] = '2019'\n",
    "# DATA['QUARTER'] = 'Q1'\n",
    "\n",
    "# DATA['YEAR'] = '2019'\n",
    "# DATA['QUARTER'] = 'Q2'\n",
    "\n",
    "# DATA['YEAR'] = '2019'\n",
    "# DATA['QUARTER'] = 'Q3'\n",
    "\n",
    "# DATA['YEAR'] = '2019'\n",
    "# DATA['QUARTER'] = 'Q4'\n",
    "\n",
    "# attrition_data = DATA\n",
    "\n",
    "\n",
    "# # attrition_data = pd.concat([QUARTER_BLOCK1, QUARTER_BLOCK2, QUARTER_BLOCK3, QUARTER_BLOCK4, QUARTER_BLOCK5, QUARTER_BLOCK6], axis=0)\n",
    "# # create flag\n",
    "# # DATA['YEAR'] = yr_qtr1[:4]\n",
    "\n",
    "# # DATA['QUARTER'] = yr_qtr1[4:] \n",
    "\n",
    "# # assigning the data\n",
    "\n",
    "# # calculate YAL and latest grade for each quarter (didn't take into account active and inactive employees)\n",
    "# attrition_data = attrition_data.sort_values(by=['Masked Code', 'YEAR', 'QUARTER'], ascending=True)\n",
    "\n",
    "# # create temp date for YAL creation\n",
    "# attrition_data['YAL_Date'] = np.where((attrition_data['EffectiveDate'].isnull()), attrition_data['DATE_OF_JOINING'], attrition_data['EffectiveDate'])\n",
    "\n",
    "# # calculate YAL\n",
    "# attrition_data['Days_at_Level'] = np.where(((attrition_data['YEAR'] == '2018') & (attrition_data['QUARTER'] == 'Q3')), \n",
    "#                                         (q1_end - attrition_data['YAL_Date']), \n",
    "#                                            np.where(((attrition_data['YEAR'] == '2018') & (attrition_data['QUARTER'] == 'Q4')),\n",
    "#                                                    (q2_end - attrition_data['YAL_Date']), \n",
    "#                                                        np.where(((attrition_data['YEAR'] == '2019') & (attrition_data['QUARTER'] == 'Q1')),\n",
    "#                                                            (q3_end - attrition_data['YAL_Date']), \n",
    "#                                                                 np.where(((attrition_data['YEAR'] == '2019') & (attrition_data['QUARTER'] == 'Q2')),\n",
    "#                                                                    (q4_end - attrition_data['YAL_Date']), \n",
    "#                                                                           np.where(((attrition_data['YEAR'] == '2019') & (attrition_data['QUARTER'] == 'Q3')),\n",
    "#                                                                                (q5_end - attrition_data['YAL_Date']), \n",
    "#                                                                                     np.where(((attrition_data['YEAR'] == '2019') & (attrition_data['QUARTER'] == 'Q4')),\n",
    "#                                                                                            (q6_end - attrition_data['YAL_Date']), \n",
    "#                                                                                                  0\n",
    "#                                                                                                     ))))))     \n",
    "\n",
    "\n",
    "# attrition_data['YAL'] = attrition_data['Days_at_Level']/np.timedelta64(1,'Y')\n",
    "\n",
    "# # calculate latest grade for quarter\n",
    "# attrition_data['LatestGrade'] = np.where((attrition_data['Grade'].isnull()), attrition_data['Masked Grade'], attrition_data['Grade'])\n",
    "\n",
    "# # promoted flag\n",
    "# attrition_data['IsPromoted'] = np.where((attrition_data['IsPromoted'] == 'Promoted'), 1 ,0)\n",
    "\n",
    "# # capping project hours for snapshot data\n",
    "# attrition_data['LatestQtrProjectDays'] = np.where((attrition_data['LatestQtrProjectDays'] > 91), 91, attrition_data['LatestQtrProjectDays'])\n",
    "# attrition_data['LatestYrProjectDays'] = np.where((attrition_data['LatestYrProjectDays'] > 365), 365, attrition_data['LatestYrProjectDays'])\n",
    "\n",
    "# # imputing outliers with median values for compensation data\n",
    "# attrition_data.loc[attrition_data['LatestYrSalaryHike'] > 100 , 'LatestYrSalaryHike'] = attrition_data['LatestYrSalaryHike'].median()\n",
    "\n",
    "# #  capping salary hike which are negative\n",
    "# attrition_data['LatestYrSalaryHike'] = np.where((attrition_data['LatestYrSalaryHike'] < 0),\n",
    "#                                                 0, attrition_data['LatestYrSalaryHike'])\n",
    "\n",
    "# # imputing nulls with 0 for LatestYrSalaryHike\n",
    "# attrition_data['LatestYrSalaryHike'] = np.where((attrition_data['LatestYrSalaryHike'].isnull()),\n",
    "#                                                 0, attrition_data['LatestYrSalaryHike'])\n",
    "\n",
    "# # replacing staff util negative values with 0\n",
    "# attrition_data['LatestYr_BILL_UTIL'][attrition_data['LatestYr_BILL_UTIL'] < 0] = 0\n",
    "# attrition_data['LatestYr_PROD_NONBILL_UTIL'][attrition_data['LatestYr_PROD_NONBILL_UTIL'] < 0] = 0\n",
    "# attrition_data['LatestYr_NONPROD_NONBILL_UTIL'][attrition_data['LatestYr_NONPROD_NONBILL_UTIL'] < 0] = 0\n",
    "\n",
    "# attrition_data['LatestQtr_BILL_UTIL'][attrition_data['LatestQtr_BILL_UTIL'] < 0] = 0\n",
    "# attrition_data['LatestQtr_PROD_NONBILL_UTIL'][attrition_data['LatestQtr_PROD_NONBILL_UTIL'] < 0] = 0\n",
    "# attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'][attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'] < 0] = 0\n",
    "\n",
    "# # replace inf with 0\n",
    "# attrition_data['LatestYr_BILL_UTIL'] = attrition_data['LatestYr_BILL_UTIL'].replace(np.inf, 0)\n",
    "# attrition_data['LatestYr_PROD_NONBILL_UTIL'] = attrition_data['LatestYr_PROD_NONBILL_UTIL'].replace(np.inf, 0)\n",
    "# attrition_data['LatestYr_NONPROD_NONBILL_UTIL'] = attrition_data['LatestYr_NONPROD_NONBILL_UTIL'].replace(np.inf, 0)\n",
    "# attrition_data['LatestQtr_BILL_UTIL'] = attrition_data['LatestQtr_BILL_UTIL'].replace(np.inf, 0)\n",
    "# attrition_data['LatestQtr_PROD_NONBILL_UTIL'] = attrition_data['LatestQtr_PROD_NONBILL_UTIL'].replace(np.inf, 0)\n",
    "# attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'] = attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'].replace(np.inf, 0)\n",
    "\n",
    "# #impute null values with 0\n",
    "# attrition_data['LatestYr_BILL_UTIL'] = attrition_data['LatestYr_BILL_UTIL'].fillna(0)\n",
    "# attrition_data['LatestYr_PROD_NONBILL_UTIL'] = attrition_data['LatestYr_PROD_NONBILL_UTIL'].fillna(0)\n",
    "# attrition_data['LatestYr_NONPROD_NONBILL_UTIL'] = attrition_data['LatestYr_NONPROD_NONBILL_UTIL'].fillna(0)\n",
    "\n",
    "# attrition_data['LatestQtr_BILL_UTIL'] = attrition_data['LatestQtr_BILL_UTIL'].fillna(0)\n",
    "# attrition_data['LatestQtr_PROD_NONBILL_UTIL'] = attrition_data['LatestQtr_PROD_NONBILL_UTIL'].fillna(0)\n",
    "# attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'] = attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'].fillna(0)\n",
    "\n",
    "# # calculating percentile\n",
    "# # staff Utilization by latest YEAR\n",
    "# attrition_data['LatestYr_PER_BILL_UTIL'] = attrition_data.groupby(['YEAR', 'QUARTER', 'Masked Function', 'Masked Department', 'LatestGrade'])['LatestYr_BILL_UTIL'].rank(pct=True, method = 'min')\n",
    "# attrition_data['LatestYr_PER_PROD_NONBILL_UTIL'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestYr_PROD_NONBILL_UTIL'].rank(pct=True, method = 'min')\n",
    "# attrition_data['LatestYr_PER_NONPROD_NONBILL_UTIL'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestYr_NONPROD_NONBILL_UTIL'].rank(pct=True, method = 'min')\n",
    "\n",
    "# # staff Utilization by latest QUARTER\n",
    "# attrition_data['LatestQtr_PER_BILL_UTIL'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestQtr_BILL_UTIL'].rank(pct=True, method = 'min')\n",
    "# attrition_data['LatestQtr_PER_PROD_NONBILL_UTIL'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestQtr_PROD_NONBILL_UTIL'].rank(pct=True, method = 'min')\n",
    "# attrition_data['LatestQtr_PER_NONPROD_NONBILL_UTIL'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestQtr_NONPROD_NONBILL_UTIL'].rank(pct=True, method = 'min')\n",
    "\n",
    "# #imputing null values of PP salary with 0\n",
    "# attrition_data['PP_LatestYr']  = np.where((attrition_data['PP_LatestYr'].isnull()), 0, attrition_data['PP_LatestYr'])\n",
    "\n",
    "# #salary variables\n",
    "# attrition_data['FP_SALARY'] = attrition_data['FP_LatestYr_Jul']\n",
    "# attrition_data['PP_SALARY'] =  attrition_data['PP_LatestYr']\n",
    "\n",
    "# # percentile salary\n",
    "# attrition_data['FP_PERCENTILE_SALARY'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['FP_LatestYr_Jul'].rank(pct=True, method = 'min')\n",
    "# attrition_data['PP_PERCENTILE_SALARY'] = attrition_data.groupby(['YEAR', 'QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['PP_LatestYr'].rank(pct=True, method = 'min')\n",
    "\n",
    "# # calculate percentile for travel frequency\n",
    "# attrition_data['PER_LatestYr_TotalTravelFreq'] = attrition_data.groupby(['YEAR','QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestYr_TotalTravelFreq'].rank(pct=True, method = 'min')\n",
    "\n",
    "# # calculate percentile for travel frequency\n",
    "# attrition_data['PER_LatestQtr_TotalTravelFreq'] = attrition_data.groupby(['YEAR','QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestQtr_TotalTravelFreq'].rank(pct=True, method = 'min')\n",
    "\n",
    "# # calculate percentile for travel duration\n",
    "# attrition_data['PER_LatestQtr_TravelDuration'] = attrition_data.groupby(['YEAR','QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestQtr_TravelDuration'].rank(pct=True, method = 'min')\n",
    "\n",
    "# # calculate percentile for travel duration\n",
    "# attrition_data['PER_LatestYr_TravelDuration'] = attrition_data.groupby(['YEAR','QUARTER','Masked Function', 'Masked Department', 'LatestGrade'])['LatestYr_TravelDuration'].rank(pct=True, method = 'min')\n",
    "\n",
    "# #YearQuarter\n",
    "# attrition_data['YearQuarter'] = attrition_data['YEAR'] + attrition_data['QUARTER']\n",
    "\n",
    "# # Leave utilised Year Quarter ratio\n",
    "# attrition_data['Leave_Util_LatestQtrYr_Ratio'] = attrition_data['LatestQtr_Leave_Util']/attrition_data['LatestYr_Leave_Util']\n",
    "\n",
    "# #  capping utilisation which are greater than 2\n",
    "# attrition_data['LatestYr_BILL_UTIL'] = np.where((attrition_data['LatestYr_BILL_UTIL'] > 2), 2, attrition_data['LatestYr_BILL_UTIL'])\n",
    "# attrition_data['LatestYr_PROD_NONBILL_UTIL'] = np.where((attrition_data['LatestYr_PROD_NONBILL_UTIL'] > 2), 2, attrition_data['LatestYr_PROD_NONBILL_UTIL'])\n",
    "# attrition_data['LatestYr_NONPROD_NONBILL_UTIL'] = np.where((attrition_data['LatestYr_NONPROD_NONBILL_UTIL'] > 2), 2, attrition_data['LatestYr_NONPROD_NONBILL_UTIL'])\n",
    "\n",
    "# attrition_data['LatestQtr_BILL_UTIL'] = np.where((attrition_data['LatestQtr_BILL_UTIL'] > 2), 2, attrition_data['LatestQtr_BILL_UTIL'])\n",
    "# attrition_data['LatestQtr_PROD_NONBILL_UTIL'] = np.where((attrition_data['LatestQtr_PROD_NONBILL_UTIL'] > 2), 2, attrition_data['LatestQtr_PROD_NONBILL_UTIL'])\n",
    "# attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'] = np.where((attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'] > 2), 2, attrition_data['LatestQtr_NONPROD_NONBILL_UTIL'])\n",
    "\n",
    "# print(\"Successfully handled Outliers\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully handled Missing Values\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attrition_data['YEARS_SINCE_MARRIAGE']  = np.where(\n",
    "                                                        ((attrition_data['MARITAL_STATUS'] == 'Single')|\n",
    "                                                         (attrition_data['MARITAL_STATUS'] == 'Widow') |\n",
    "                                                         (attrition_data['MARITAL_STATUS'] == 'Divorced')), \n",
    "                                                                0, \n",
    "                                                                    attrition_data['YEARS_SINCE_MARRIAGE'])                                                      \n",
    "\n",
    "attrition_data['MARITAL_STATUS'] = np.where((attrition_data['MARITAL_STATUS'].isnull()), 'Not Available', attrition_data['MARITAL_STATUS'])\n",
    "\n",
    "attrition_data['YEARS_SINCE_MARRIAGE'] = np.where((attrition_data['YEARS_SINCE_MARRIAGE'] >= 0), attrition_data['YEARS_SINCE_MARRIAGE'], 0)\n",
    "\n",
    "attrition_data['IsPromoted']  = np.where((attrition_data['IsPromoted'].isnull()), 'Non-Promoted', attrition_data['IsPromoted'])\n",
    "\n",
    "attrition_data['TOTAL_WORK_EXP'] = np.where((attrition_data['IS_EXPERIENCED'] == 0), attrition_data['YEARS_AT_COMPANY'], attrition_data['TOTAL_WORK_EXP'])\n",
    "\n",
    "attrition_data['PRIOR_EXP'] = np.where((attrition_data['IS_EXPERIENCED'] == 0), 0, attrition_data['PRIOR_EXP'])\n",
    "\n",
    "attrition_data['PRIOR_AVG_TENURE'] = np.where((attrition_data['IS_EXPERIENCED'] == 0), 0, attrition_data['PRIOR_AVG_TENURE'])\n",
    "\n",
    "attrition_data['NUM_PRIOR_COMPANIES'] = np.where((attrition_data['IS_EXPERIENCED'] == 0), 0, attrition_data['NUM_PRIOR_COMPANIES'])\n",
    "\n",
    "attrition_data['LatestQtrProjectDays'] = np.where((attrition_data['LatestQtrProjectDays'].isnull()), 0, attrition_data['LatestQtrProjectDays'])\n",
    "\n",
    "attrition_data['LatestYrProjectDays'] = np.where((attrition_data['LatestYrProjectDays'].isnull()), 0, attrition_data['LatestYrProjectDays'])\n",
    "\n",
    "attrition_data['LatestQtr_Leave_Util'] = np.where((attrition_data['LatestQtr_Leave_Util'].isnull()), 0, attrition_data['LatestQtr_Leave_Util'])\n",
    "\n",
    "attrition_data['LatestYr_Leave_Util'] = np.where((attrition_data['LatestYr_Leave_Util'].isnull()), 0, attrition_data['LatestYr_Leave_Util'])\n",
    "\n",
    "attrition_data['MLPL_flag'] = np.where((attrition_data['MLPL_flag'].isnull()), 0, attrition_data['MLPL_flag'])\n",
    "\n",
    "attrition_data['BL_flag'] = np.where((attrition_data['BL_flag'].isnull()), 0, attrition_data['BL_flag'])\n",
    "\n",
    "attrition_data['RTL_flag'] = np.where((attrition_data['RTL_flag'].isnull()), 0, attrition_data['RTL_flag'])\n",
    "\n",
    "attrition_data['LatestYr_LWP_Leave_Util'] = np.where((attrition_data['LWP_leaves'].isnull()), 0, attrition_data['LWP_leaves'])\n",
    "\n",
    "attrition_data['LatestQtr_snapshot_composite_score'] = np.where((attrition_data['LatestQtr_snapshot_composite_score'].isnull()), 0, attrition_data['LatestQtr_snapshot_composite_score'])\n",
    "\n",
    "attrition_data['LatestYr_snapshot_composite_score'] = np.where((attrition_data['LatestYr_snapshot_composite_score'].isnull()), 0, attrition_data['LatestYr_snapshot_composite_score'])\n",
    "\n",
    "attrition_data['IsPromoted_LatestQtr'] =  np.where((attrition_data['IsPromoted_LatestQtr'].isnull()), 0, attrition_data['IsPromoted_LatestQtr'])\n",
    "\n",
    "attrition_data['IsPromoted_PreviousQtr'] =  np.where((attrition_data['IsPromoted_PreviousQtr'].isnull()), 0, attrition_data['IsPromoted_PreviousQtr'])\n",
    "\n",
    "attrition_data['LatestQtr_LWP_Leave_Util'] = np.where((attrition_data['LatestQtr_LWP_Leave_Util'].isnull()), 0, attrition_data['LatestQtr_LWP_Leave_Util'])\n",
    "\n",
    "attrition_data['EmpPerformanceRating'] = np.where((attrition_data['EmpPerformanceRating'].isnull()), 'Not Eligible', attrition_data['EmpPerformanceRating'])\n",
    "\n",
    "attrition_data['RMPerformanceRating'] = np.where((attrition_data['RMPerformanceRating'].isnull()), 'Not Eligible', attrition_data['RMPerformanceRating'])\n",
    "\n",
    "attrition_data['Leave_Util_LatestQtrYr_Ratio'] =np.where((attrition_data['Leave_Util_LatestQtrYr_Ratio'].isnull()), 0, attrition_data['Leave_Util_LatestQtrYr_Ratio'])\n",
    "\n",
    "print(\"Successfully handled Missing Values\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully included all the variables into final data\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Subsetting final data\n",
    "attrition_data = attrition_data[['Masked Code', 'YEAR', 'QUARTER','YearQuarter','Masked Gender', 'MARITAL_STATUS','Masked Entity','Masked Status', 'Masked Department', 'DATE_OF_JOINING','Masked RM ID',\n",
    "                            'Masked Function','PARTNER_NAME','REPORTING_TO', 'LatestGrade', 'NOTICE_PERIOD_IN_DAYS','AGE','YEARS_SINCE_MARRIAGE', \n",
    "                            'Masked Location', 'IsSameState','YEARS_AT_COMPANY', 'IsPromoted','IsPromoted_LatestQtr','IsPromoted_PreviousQtr','YAL',\n",
    "                            'LatestYr_BILL_UTIL', 'LatestYr_PROD_NONBILL_UTIL', 'LatestYr_NONPROD_NONBILL_UTIL',\n",
    "                            'LatestQtr_BILL_UTIL', 'LatestQtr_PROD_NONBILL_UTIL', 'LatestQtr_NONPROD_NONBILL_UTIL',\n",
    "                            'LatestYr_PER_BILL_UTIL', 'LatestYr_PER_PROD_NONBILL_UTIL', 'LatestYr_PER_NONPROD_NONBILL_UTIL',\n",
    "                            'LatestQtr_PER_BILL_UTIL', 'LatestQtr_PER_PROD_NONBILL_UTIL', 'LatestQtr_PER_NONPROD_NONBILL_UTIL',\n",
    "                            'IS_EXPERIENCED','TOTAL_WORK_EXP','PRIOR_EXP','PRIOR_AVG_TENURE','NUM_PRIOR_COMPANIES',\n",
    "                            'LAST5YRS_COMP_WORKED','LatestQtrTechLearnHours','LatestQtrProfLeadDevLearnHours','LatestYrTechLearnHours','LatestYrProfLeadDevLearnHours',\n",
    "                            'LatestQtrTechNumCourses','LatestQtrProfLeadDevNumCourses','LatestYrTechNumCourses','LatestYrProfLeadDevNumCourses',\n",
    "                            'NumOfRewards','NumOfRecognitions','TotalRnR','IsRewarded','IsRecognized','Amount(Rs)','LatestQtrProjectDays',\n",
    "                            'LatestYrProjectDays','LatestQtrPerfSnapshot','LatestQtrOtherSnapshot','LatestQtrTotalSnapshot',\n",
    "                            'LatestYrPerfSnapshot','LatestYrOtherSnapshot','LatestYrTotalSnapshot','LatestQtr_snapshot_composite_score','LatestYr_snapshot_composite_score','LatestQtr_Leave_Util',\n",
    "                            'LatestYr_Leave_Util','Leave_Util_LatestQtrYr_Ratio','MLPL_flag','BL_flag','RTL_flag','LatestYr_LWP_Leave_Util','LatestQtr_LWP_Leave_Util','CountOfRejections','FP_SALARY','PP_SALARY','FP_PERCENTILE_SALARY',\n",
    "                            'PP_PERCENTILE_SALARY','LatestYrSalaryHike','EmpPerformanceRating','RMPerformanceRating',\n",
    "                            'LatestQtr_HasDomesticTravel', 'LatestQtr_HasInternationalTravel', 'LatestQtr_TotalTravelFreq', 'LatestQtr_TravelDuration','LatestYr_TotalTravelFreq', 'LatestYr_TravelDuration',\n",
    "                            'PER_LatestQtr_TotalTravelFreq', 'PER_LatestQtr_TravelDuration',\n",
    "                            'PER_LatestYr_TotalTravelFreq', 'PER_LatestYr_TravelDuration', 'Attrition']]\n",
    "\n",
    "# rename the columns\n",
    "attrition_data = attrition_data.rename(columns={\"Masked Code\": \"Masked_Code\",\n",
    "                                                        \"Masked Gender\": \"Masked_Gender\",\n",
    "                                                        \"Masked Status\": \"Masked_Status\",\n",
    "                                                        \"Masked Entity\": \"Masked_Entity\",\n",
    "                                                        \"Masked Function\": \"Masked_Function\",\n",
    "                                                        \"Masked Location\": \"Masked_Location\",\n",
    "                                                        \"Masked RM ID\" : \"Masked_RM_ID\",\n",
    "                                                        \"Amount(Rs)\": \"RewardAmount\"})\n",
    "\n",
    "\n",
    "print(\"Successfully included all the variables into final data\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DATA.loc[DATA['Attrition']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Masked_Code',\n",
       " 'YEAR',\n",
       " 'QUARTER',\n",
       " 'YearQuarter',\n",
       " 'Masked_Gender',\n",
       " 'MARITAL_STATUS',\n",
       " 'Masked_Entity',\n",
       " 'Masked_Status',\n",
       " 'Masked Department',\n",
       " 'DATE_OF_JOINING',\n",
       " 'Masked_RM_ID',\n",
       " 'Masked_Function',\n",
       " 'PARTNER_NAME',\n",
       " 'REPORTING_TO',\n",
       " 'LatestGrade',\n",
       " 'NOTICE_PERIOD_IN_DAYS',\n",
       " 'AGE',\n",
       " 'YEARS_SINCE_MARRIAGE',\n",
       " 'Masked_Location',\n",
       " 'IsSameState',\n",
       " 'YEARS_AT_COMPANY',\n",
       " 'IsPromoted',\n",
       " 'IsPromoted_LatestQtr',\n",
       " 'IsPromoted_PreviousQtr',\n",
       " 'YAL',\n",
       " 'LatestYr_BILL_UTIL',\n",
       " 'LatestYr_PROD_NONBILL_UTIL',\n",
       " 'LatestYr_NONPROD_NONBILL_UTIL',\n",
       " 'LatestQtr_BILL_UTIL',\n",
       " 'LatestQtr_PROD_NONBILL_UTIL',\n",
       " 'LatestQtr_NONPROD_NONBILL_UTIL',\n",
       " 'LatestYr_PER_BILL_UTIL',\n",
       " 'LatestYr_PER_PROD_NONBILL_UTIL',\n",
       " 'LatestYr_PER_NONPROD_NONBILL_UTIL',\n",
       " 'LatestQtr_PER_BILL_UTIL',\n",
       " 'LatestQtr_PER_PROD_NONBILL_UTIL',\n",
       " 'LatestQtr_PER_NONPROD_NONBILL_UTIL',\n",
       " 'IS_EXPERIENCED',\n",
       " 'TOTAL_WORK_EXP',\n",
       " 'PRIOR_EXP',\n",
       " 'PRIOR_AVG_TENURE',\n",
       " 'NUM_PRIOR_COMPANIES',\n",
       " 'LAST5YRS_COMP_WORKED',\n",
       " 'LatestQtrTechLearnHours',\n",
       " 'LatestQtrProfLeadDevLearnHours',\n",
       " 'LatestYrTechLearnHours',\n",
       " 'LatestYrProfLeadDevLearnHours',\n",
       " 'LatestQtrTechNumCourses',\n",
       " 'LatestQtrProfLeadDevNumCourses',\n",
       " 'LatestYrTechNumCourses',\n",
       " 'LatestYrProfLeadDevNumCourses',\n",
       " 'NumOfRewards',\n",
       " 'NumOfRecognitions',\n",
       " 'TotalRnR',\n",
       " 'IsRewarded',\n",
       " 'IsRecognized',\n",
       " 'RewardAmount',\n",
       " 'LatestQtrProjectDays',\n",
       " 'LatestYrProjectDays',\n",
       " 'LatestQtrPerfSnapshot',\n",
       " 'LatestQtrOtherSnapshot',\n",
       " 'LatestQtrTotalSnapshot',\n",
       " 'LatestYrPerfSnapshot',\n",
       " 'LatestYrOtherSnapshot',\n",
       " 'LatestYrTotalSnapshot',\n",
       " 'LatestQtr_snapshot_composite_score',\n",
       " 'LatestYr_snapshot_composite_score',\n",
       " 'LatestQtr_Leave_Util',\n",
       " 'LatestYr_Leave_Util',\n",
       " 'Leave_Util_LatestQtrYr_Ratio',\n",
       " 'MLPL_flag',\n",
       " 'BL_flag',\n",
       " 'RTL_flag',\n",
       " 'LatestYr_LWP_Leave_Util',\n",
       " 'LatestQtr_LWP_Leave_Util',\n",
       " 'CountOfRejections',\n",
       " 'FP_SALARY',\n",
       " 'PP_SALARY',\n",
       " 'FP_PERCENTILE_SALARY',\n",
       " 'PP_PERCENTILE_SALARY',\n",
       " 'LatestYrSalaryHike',\n",
       " 'EmpPerformanceRating',\n",
       " 'RMPerformanceRating',\n",
       " 'LatestQtr_HasDomesticTravel',\n",
       " 'LatestQtr_HasInternationalTravel',\n",
       " 'LatestQtr_TotalTravelFreq',\n",
       " 'LatestQtr_TravelDuration',\n",
       " 'LatestYr_TotalTravelFreq',\n",
       " 'LatestYr_TravelDuration',\n",
       " 'PER_LatestQtr_TotalTravelFreq',\n",
       " 'PER_LatestQtr_TravelDuration',\n",
       " 'PER_LatestYr_TotalTravelFreq',\n",
       " 'PER_LatestYr_TravelDuration',\n",
       " 'Attrition']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(attrition_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attrition_data.drop(['Masked_RM_ID','PARTNER_NAME','REPORTING_TO'], axis = 1) \n",
    "attrition_data = attrition_data[['Masked_Code'\n",
    ",'YEAR','Masked_RM_ID','PARTNER_NAME','REPORTING_TO'\n",
    ",'QUARTER'\n",
    ",'YearQuarter'\n",
    ",'Masked_Gender'\n",
    ",'MARITAL_STATUS'\n",
    ",'Masked_Entity'\n",
    ",'Masked_Status'\n",
    ",'Masked Department'\n",
    ",'DATE_OF_JOINING'\n",
    ",'Masked_Function'\n",
    ",'LatestGrade'\n",
    ",'NOTICE_PERIOD_IN_DAYS'\n",
    ",'AGE'\n",
    ",'YEARS_SINCE_MARRIAGE'\n",
    ",'Masked_Location'\n",
    ",'IsSameState'\n",
    ",'YEARS_AT_COMPANY'\n",
    ",'IsPromoted'\n",
    ",'IsPromoted_LatestQtr'\n",
    ",'IsPromoted_PreviousQtr'\n",
    ",'YAL'\n",
    ",'LatestYr_BILL_UTIL'\n",
    ",'LatestYr_PROD_NONBILL_UTIL'\n",
    ",'LatestYr_NONPROD_NONBILL_UTIL'\n",
    ",'LatestQtr_BILL_UTIL'\n",
    ",'LatestQtr_PROD_NONBILL_UTIL'\n",
    ",'LatestQtr_NONPROD_NONBILL_UTIL'\n",
    ",'LatestYr_PER_BILL_UTIL'\n",
    ",'LatestYr_PER_PROD_NONBILL_UTIL'\n",
    ",'LatestYr_PER_NONPROD_NONBILL_UTIL'\n",
    ",'LatestQtr_PER_BILL_UTIL'\n",
    ",'LatestQtr_PER_PROD_NONBILL_UTIL'\n",
    ",'LatestQtr_PER_NONPROD_NONBILL_UTIL'\n",
    ",'IS_EXPERIENCED'\n",
    ",'TOTAL_WORK_EXP'\n",
    ",'PRIOR_EXP'\n",
    ",'PRIOR_AVG_TENURE'\n",
    ",'NUM_PRIOR_COMPANIES'\n",
    ",'LAST5YRS_COMP_WORKED'\n",
    ",'LatestQtrTechLearnHours'\n",
    ",'LatestQtrProfLeadDevLearnHours'\n",
    ",'LatestYrTechLearnHours'\n",
    ",'LatestYrProfLeadDevLearnHours'\n",
    ",'LatestQtrTechNumCourses'\n",
    ",'LatestQtrProfLeadDevNumCourses'\n",
    ",'LatestYrTechNumCourses'\n",
    ",'LatestYrProfLeadDevNumCourses'\n",
    ",'NumOfRewards'\n",
    ",'NumOfRecognitions'\n",
    ",'TotalRnR'\n",
    ",'IsRewarded'\n",
    ",'IsRecognized'\n",
    ",'RewardAmount'\n",
    ",'LatestQtrProjectDays'\n",
    ",'LatestYrProjectDays'\n",
    ",'LatestQtrPerfSnapshot'\n",
    ",'LatestQtrOtherSnapshot'\n",
    ",'LatestQtrTotalSnapshot'\n",
    ",'LatestYrPerfSnapshot'\n",
    ",'LatestYrOtherSnapshot'\n",
    ",'LatestYrTotalSnapshot'\n",
    ",'LatestQtr_snapshot_composite_score'\n",
    ",'LatestYr_snapshot_composite_score'\n",
    ",'LatestQtr_Leave_Util'\n",
    ",'LatestYr_Leave_Util'\n",
    ",'Leave_Util_LatestQtrYr_Ratio'\n",
    ",'MLPL_flag'\n",
    ",'BL_flag'\n",
    ",'RTL_flag'\n",
    ",'LatestYr_LWP_Leave_Util'\n",
    ",'LatestQtr_LWP_Leave_Util'\n",
    ",'CountOfRejections'\n",
    ",'FP_SALARY'\n",
    ",'PP_SALARY'\n",
    ",'FP_PERCENTILE_SALARY'\n",
    ",'PP_PERCENTILE_SALARY'\n",
    ",'LatestYrSalaryHike'\n",
    ",'EmpPerformanceRating'\n",
    ",'RMPerformanceRating'\n",
    ",'LatestQtr_HasDomesticTravel'\n",
    ",'LatestQtr_HasInternationalTravel'\n",
    ",'LatestQtr_TotalTravelFreq'\n",
    ",'LatestQtr_TravelDuration'\n",
    ",'LatestYr_TotalTravelFreq'\n",
    ",'LatestYr_TravelDuration'\n",
    ",'PER_LatestQtr_TotalTravelFreq'\n",
    ",'PER_LatestQtr_TravelDuration'\n",
    ",'PER_LatestYr_TotalTravelFreq'\n",
    ",'PER_LatestYr_TravelDuration'\n",
    ",'Attrition'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_data_F002 = attrition_data[attrition_data['Masked_Function']== \"F002\"]\n",
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code/Code/Mapping Table/')\n",
    "# get the department group for F002\n",
    "F002_Department_Group = pd.read_excel('F002 Department Group.xlsx', sheet_name = \"Sheet1\")\n",
    "attrition_data_F002 = pd.merge(attrition_data_F002, F002_Department_Group, on='Masked Department', how='left')\n",
    "\n",
    "\n",
    "# get the department group for F005\n",
    "\n",
    "#dividing data based on function\n",
    "attrition_data_F005 = attrition_data[attrition_data['Masked_Function']== \"F005\"]\n",
    "attrition_data_F005['Masked_Department_Group'] = np.where((attrition_data_F005['Masked Department'] == \"SL080\"), \n",
    "                                                                    \"SL052\", \n",
    "                                                                            attrition_data_F005['Masked Department'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(attrition_data_F002['Masked Department'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit\n",
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\')\n",
    "writer = pd.ExcelWriter('Feature_Universe_2020Q2_F002.xlsx', engine='xlsxwriter', datetime_format='yyyy-mm-dd')\n",
    "attrition_data_F002.to_excel(writer, index= False)\n",
    "writer.save()\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\')\n",
    "writer = pd.ExcelWriter('Feature_Universe_2020Q2_F005.xlsx', engine='xlsxwriter', datetime_format='yyyy-mm-dd')\n",
    "attrition_data_F005.to_excel(writer, index= False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-57-9a4cc416d1ea>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-57-9a4cc416d1ea>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    print'dropoff\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "print'dropoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## converting F005 few variables to percentiles\n",
    "# attrition_data['PER_LatestQtrProjectDays'] = attrition_data.groupby(['YEAR','QUARTER','Masked_Function', 'Masked Department', 'LatestGrade'])['LatestQtrProjectDays'].rank(pct=True, method = 'min')\n",
    "# attrition_data['PER_YAL'] = attrition_data.groupby(['YEAR','QUARTER','Masked_Function', 'Masked Department', 'LatestGrade'])['YAL'].rank(pct=True, method = 'min')\n",
    "# attrition_data['PER_LatestQtr_snapshot_composite_score'] = attrition_data.groupby(['YEAR','QUARTER','Masked_Function', 'Masked Department', 'LatestGrade'])['LatestQtr_snapshot_composite_score'].rank(pct=True, method = 'min')\n",
    "# attrition_data['PER_LatestQtrProfLeadDevLearnHours'] = attrition_data.groupby(['YEAR','QUARTER','Masked_Function', 'Masked Department', 'LatestGrade'])['LatestQtrProfLeadDevLearnHours'].rank(pct=True, method = 'min')\n",
    "# attrition_data['PER_LatestQtrProfLeadDevNumCourses'] = attrition_data.groupby(['YEAR','QUARTER','Masked_Function', 'Masked Department', 'LatestGrade'])['LatestQtrProfLeadDevNumCourses'].rank(pct=True, method = 'min')\n",
    "# attrition_data['PER_LatestQtrPerfSnapshot'] = attrition_data.groupby(['YEAR','QUARTER','Masked_Function', 'Masked Department', 'LatestGrade'])['LatestQtrPerfSnapshot'].rank(pct=True, method = 'min')\n",
    "# attrition_data['PER_LatestQtrTechLearnHours'] = attrition_data.groupby(['YEAR','QUARTER','Masked_Function', 'Masked Department', 'LatestGrade'])['LatestQtrTechLearnHours'].rank(pct=True, method = 'min')\n",
    "# attrition_data['PER_Leave_Util_LatestQtrYr_Ratio'] = attrition_data.groupby(['YEAR','Masked_Function', 'Masked Department', 'LatestGrade'])['Leave_Util_LatestQtrYr_Ratio'].rank(pct=True, method = 'min')\n",
    "# attrition_data['PER_LatestYrPerfSnapshot'] = attrition_data.groupby(['YEAR','QUARTER','Masked_Function', 'Masked Department', 'LatestGrade'])['LatestYrPerfSnapshot'].rank(pct=True, method = 'min')\n",
    "# attrition_data['PER_LatestYr_snapshot_composite_score'] = attrition_data.groupby(['YEAR','QUARTER','Masked_Function', 'Masked Department', 'LatestGrade'])['LatestYr_snapshot_composite_score'].rank(pct=True, method = 'min')\n",
    "# attrition_data['PER_LatestYrProjectDays'] = attrition_data.groupby(['YEAR','QUARTER','Masked_Function', 'Masked Department', 'LatestGrade'])['LatestYrProjectDays'].rank(pct=True, method = 'min')\n",
    "\n",
    "# print(\"Successfully converted variables into percentile required for Model\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yr_qtr1[4:])\n",
    "DATA['YEAR'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code/Code/Mapping Table/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for F002 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dividing data based on function\n",
    "attrition_data_F002 = attrition_data[attrition_data['Masked_Function']== \"F002\"]\n",
    "\n",
    "# get the department group for F002\n",
    "F002_Department_Group = pd.read_excel('F002 Department Group.xlsx', sheet_name = \"Sheet1\")\n",
    "attrition_data_F002 = pd.merge(attrition_data_F002, F002_Department_Group, on='Masked Department', how='left')\n",
    "\n",
    "attrition_data_F002  =attrition_data_F002[['Masked_Code'\n",
    "                                            ,'YearQuarter'\n",
    "                                            ,'AGE'\n",
    "                                            ,'DATE_OF_JOINING'\n",
    "                                            ,'Masked_Gender'\n",
    "                                            ,'Masked Department'\n",
    "                                            ,'Masked_Function'\n",
    "                                            ,'PARTNER_NAME'\n",
    "                                            ,'REPORTING_TO'\n",
    "                                            ,'Masked_RM_ID'\n",
    "                                            ,'Masked_Location'\n",
    "                                            ,'EmpPerformanceRating'\n",
    "                                            ,'LatestQtrProjectDays'\n",
    "                                            ,'YAL'\n",
    "                                            ,'QUARTER'\n",
    "                                            ,'PP_SALARY'\n",
    "                                            ,'LatestYrProjectDays'\n",
    "                                            ,'LatestQtrProfLeadDevLearnHours'\n",
    "                                            ,'Leave_Util_LatestQtrYr_Ratio'\n",
    "                                            ,'PP_PERCENTILE_SALARY'\n",
    "                                            ,'LatestQtr_snapshot_composite_score'\n",
    "                                            ,'LatestQtrTechLearnHours'\n",
    "                                            ,'LatestQtrProfLeadDevNumCourses'\n",
    "                                            ,'LatestQtr_PROD_NONBILL_UTIL'\n",
    "                                            ,'LatestYr_BILL_UTIL']] \n",
    "\n",
    "# print the features with missing values in log\n",
    "check_missing_list_F002 = [col for col in attrition_data_F002.columns if attrition_data_F002[col].isnull().any()]\n",
    "if len(check_missing_list_F002)> 0:\n",
    "    log.write(\"F002 Features with missing values:\\n\")\n",
    "    for i in check_missing_list_F002:\n",
    "        log.write(i + \"\\n\")\n",
    "        print(i) \n",
    "else:\n",
    "    log.write(\"F002 Features has no missing values:\\n\")\n",
    "\n",
    "# check for missing values in F002 dataset\n",
    "features_F002 =  ['Masked_Code'\n",
    "                ,'YearQuarter'\n",
    "                ,'AGE'\n",
    "                ,'DATE_OF_JOINING'\n",
    "                ,'Masked Department'\n",
    "                ,'Masked_Function'\n",
    "                ,'PARTNER_NAME'\n",
    "                ,'REPORTING_TO'\n",
    "                ,'Masked_RM_ID'\n",
    "                ,'Masked_Location'\n",
    "                ,'EmpPerformanceRating'\n",
    "                ,'LatestQtrProjectDays'\n",
    "                ,'YAL'\n",
    "                ,'QUARTER'\n",
    "                ,'PP_SALARY'\n",
    "                ,'LatestYrProjectDays'\n",
    "                ,'LatestQtrProfLeadDevLearnHours'\n",
    "                ,'Leave_Util_LatestQtrYr_Ratio'\n",
    "                ,'PP_PERCENTILE_SALARY'\n",
    "                ,'LatestQtr_snapshot_composite_score'\n",
    "                ,'LatestQtrTechLearnHours'\n",
    "                ,'LatestQtrProfLeadDevNumCourses'\n",
    "                ,'LatestQtr_PROD_NONBILL_UTIL'\n",
    "                ,'LatestYr_BILL_UTIL']\n",
    "\n",
    "check_missing_values_F002 = pd.DataFrame(attrition_data_F002[features_F002].isnull().sum())\n",
    "check_missing_values_F002.columns = ['missing_count']\n",
    "check_missing_values_F002 = check_missing_values_F002.sort_values(by=['missing_count'], ascending=False)\n",
    "if ((check_missing_values_F002.iloc[0, 0]) >0):\n",
    "    print(\"Data Quality Issue - Missing Values in F002 model variables\")\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Test\\\\')\n",
    "\n",
    "# exporting data into excel for F002 model\n",
    "writer = pd.ExcelWriter('Attrition_Analytics_F002.xlsx', engine='xlsxwriter', datetime_format='yyyy-mm-dd')\n",
    "attrition_data_F002.to_excel(writer, index= False)\n",
    "writer.save()\n",
    "\n",
    "print(\"Successfully appended required variables and exported data file for F002 model\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for F005 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dividing data based on function\n",
    "attrition_data_F005 = attrition_data[attrition_data['Masked_Function']== \"F005\"]\n",
    "\n",
    "# get the department group for F005\n",
    "attrition_data_F005['Masked_Department_Group'] = np.where((attrition_data_F005['Masked Department'] == \"SL080\"), \n",
    "                                                                    \"SL052\", \n",
    "                                                                            attrition_data_F005['Masked Department'])\n",
    "\n",
    "\n",
    "attrition_data_F005 = attrition_data_F005[['Masked_Code'\n",
    "                                            ,'YearQuarter'\n",
    "                                            ,'AGE'\n",
    "                                            ,'DATE_OF_JOINING'\n",
    "                                            ,'Masked_Gender'\n",
    "                                            ,'Masked Department'\n",
    "                                            ,'Masked_Function'\n",
    "                                            ,'PARTNER_NAME'                                             \n",
    "                                            ,'REPORTING_TO'\n",
    "                                            ,'Masked_RM_ID'\n",
    "                                            ,'Masked_Location'\n",
    "                                            ,'EmpPerformanceRating'\n",
    "                                            ,'QUARTER'\n",
    "                                            ,'PER_LatestQtrProjectDays'\n",
    "                                            ,'YEARS_AT_COMPANY'\n",
    "                                            ,'PP_SALARY'\n",
    "                                            ,'PP_PERCENTILE_SALARY'\n",
    "                                            ,'LatestYrProjectDays'\n",
    "                                            ,'PER_LatestQtrProfLeadDevLearnHours'\n",
    "                                            ,'PER_LatestQtrTechLearnHours'\n",
    "                                            ,'LatestYr_snapshot_composite_score'\n",
    "                                            ,'PER_LatestYrPerfSnapshot']]\n",
    "\n",
    "# print the features with missing values in log\n",
    "check_missing_list_F005 = [col for col in attrition_data_F005.columns if attrition_data_F005[col].isnull().any()]\n",
    "if len(check_missing_list_F005)> 0:\n",
    "    print(\"F005 Features with missing values:\\n\")\n",
    "    for i in check_missing_list_F005:\n",
    "        log.write(i + \"\\n\")\n",
    "        print(i) \n",
    "else:\n",
    "    print(\"F005 Features has no missing values:\\n\")\n",
    "\n",
    "# check for missing values in F005 dataset\n",
    "features_F005 =  ['Masked_Code'\n",
    "                ,'YearQuarter'\n",
    "                ,'AGE'\n",
    "                ,'DATE_OF_JOINING'\n",
    "                ,'Masked Department'\n",
    "                ,'Masked_Function'\n",
    "                ,'PARTNER_NAME'                                             \n",
    "                ,'REPORTING_TO'\n",
    "                ,'Masked_RM_ID'\n",
    "                ,'Masked_Location'\n",
    "                ,'EmpPerformanceRating'\n",
    "                ,'QUARTER'\n",
    "                ,'PER_LatestQtrProjectDays'\n",
    "                ,'YEARS_AT_COMPANY'\n",
    "                ,'PP_SALARY'\n",
    "                ,'PP_PERCENTILE_SALARY'\n",
    "                ,'LatestYrProjectDays'\n",
    "                ,'PER_LatestQtrProfLeadDevLearnHours'\n",
    "                ,'PER_LatestQtrTechLearnHours'\n",
    "                ,'LatestYr_snapshot_composite_score'\n",
    "                ,'PER_LatestYrPerfSnapshot']\n",
    "\n",
    "check_missing_values_F005 = pd.DataFrame(attrition_data_F005[features_F005].isnull().sum())\n",
    "check_missing_values_F005.columns = ['missing_count']\n",
    "check_missing_values_F005 = check_missing_values_F005.sort_values(by=['missing_count'], ascending=False)\n",
    "if ((check_missing_values_F005.iloc[0, 0]) >0):\n",
    "    print(\"Data Quality Issue - Missing Values in F005 model variables\")\n",
    "    log.write(\"Data Quality Issue - Missing Values in F005 model variables\\r\\n\")\n",
    "    log.close()\n",
    "    sys.exit(1) \n",
    "\n",
    "# exporting data into excel for F005 model\n",
    "writer = pd.ExcelWriter('Attrition_Analytics_F005.xlsx', engine='xlsxwriter', datetime_format='yyyy-mm-dd')\n",
    "attrition_data_F005.to_excel(writer, index= False)\n",
    "writer.save()\n",
    "\n",
    "print(\"Successfully appended required variables and exported data file for F005 model\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\KNAGENDRA\\\\Desktop\\\\Attrition_model\\\\Data and Code/Code/Mapping Table/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Check for Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Distribution Check for F002 Model Features\n",
    "F002_distribution= pd.read_excel('Train_Distribution.xlsx', sheet_name = \"F002\")\n",
    "\n",
    "F002_stats  = attrition_data_F002[['LatestQtrProjectDays'\n",
    "                                    ,'YAL'\n",
    "                                    ,'PP_SALARY'\n",
    "                                    ,'LatestYrProjectDays'\n",
    "                                    ,'LatestQtrProfLeadDevLearnHours'\n",
    "                                    ,'Leave_Util_LatestQtrYr_Ratio'\n",
    "                                    ,'PP_PERCENTILE_SALARY'\n",
    "                                    ,'LatestQtr_snapshot_composite_score'\n",
    "                                    ,'LatestQtrTechLearnHours'\n",
    "                                    ,'LatestQtrProfLeadDevNumCourses'\n",
    "                                    ,'LatestQtr_PROD_NONBILL_UTIL'\n",
    "                                    ,'LatestYr_BILL_UTIL']] \n",
    "\n",
    "\n",
    "F002_stats = F002_stats.describe().loc[['mean','std']]\n",
    "F002_stats = F002_stats.transpose()\n",
    "F002_stats = F002_stats.reset_index()\n",
    "F002_stats.columns = ['Feature', 'Scoring_Data_Mean', 'Scoring_Data_SD']\n",
    "F002_stats = pd.merge(F002_distribution,F002_stats, on = 'Feature', how = 'left')\n",
    "\n",
    "F002_stats['IsDistributionDiff'] = np.where(((F002_stats['Scoring_Data_Mean']/F002_stats['Train_Mean']) >= 0.75) &\n",
    "                                              ((F002_stats['Scoring_Data_Mean']/F002_stats['Train_Mean']) <= 1.25), 0, 1)\n",
    "\n",
    "if(sum(F002_stats['IsDistributionDiff'])/len(F002_stats) > 0.25):\n",
    "    print(\"Different Distribution - F002 Model needs to be re-trained\\r\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Distribution Check for F005 Model Features\n",
    "F005_distribution= pd.read_excel('Train_Distribution.xlsx', sheet_name = \"F005\")\n",
    "\n",
    "F005_stats  = attrition_data_F005[['PER_LatestQtrProjectDays'\n",
    "                                    ,'YEARS_AT_COMPANY'\n",
    "                                    ,'PP_SALARY'\n",
    "                                    ,'PP_PERCENTILE_SALARY'\n",
    "                                    ,'LatestYrProjectDays'\n",
    "                                    ,'PER_LatestQtrProfLeadDevLearnHours'\n",
    "                                    ,'PER_LatestQtrTechLearnHours'\n",
    "                                    ,'LatestYr_snapshot_composite_score'\n",
    "                                    ,'PER_LatestYrPerfSnapshot']] \n",
    "\n",
    "F005_stats = F005_stats.describe().loc[['mean','std']]\n",
    "F005_stats = F005_stats.transpose()\n",
    "F005_stats = F005_stats.reset_index()\n",
    "F005_stats.columns = ['Feature', 'Scoring_Data_Mean', 'Scoring_Data_SD']\n",
    "F005_stats = pd.merge(F005_distribution,F005_stats, on = 'Feature', how = 'left')\n",
    "\n",
    "F005_stats['IsDistributionDiff'] = np.where(((F005_stats['Scoring_Data_Mean']/F005_stats['Train_Mean']) >= 0.75) &\n",
    "                                              ((F005_stats['Scoring_Data_Mean']/F005_stats['Train_Mean']) <= 1.25), 0, 1)\n",
    "\n",
    "if(sum(F005_stats['IsDistributionDiff'])/len(F005_stats) > 0.25):\n",
    "    print(\"Different Distribution - F005 Model needs to be re-trained\\r\\n\")\n",
    "    log.write(\"Different Distribution - F005 Model needs to be re-trained\\r\\n\")\n",
    "else:\n",
    "    print(\"Similar Distribution - F005 Model need not be re-trained\\r\\n\")\n",
    "    log.write(\"Similar Distribution - F005 Model need not be re-trained\\r\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.write(\"Attrition Analytics ETL Executed\\r\\n\")\n",
    "log.write(\"\\n\")\n",
    "log.write(\"\\n\")\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
